{"version":3,"sources":["App.module.scss","assets/FigOne.png","App.js","serviceWorker.js","index.js"],"names":["module","exports","Body","props","className","classNames","styles","mainBody","Left","menuActive","left","leftClose","leftImage","src","alt","leftContent","entryTop","to","exact","path","style","paddingLeft","entry","marginRight","href","paddingRight","icon","faEnvelope","faTwitter","faGithub","Sketch","createP5Sketch","p5Instance","componentProps","wrapperElement","pause","setup","createCanvas","noFill","background","stroke","frameRate","mousePressed","draw","push","translate","width","height","rotate","frameCount","circleResolution","parseInt","map","mouseY","radius","mouseX","angle","Math","PI","beginShape","i","x","cos","y","sin","strokeWeight","vertex","endShape","CLOSE","pop","Right","right","rightClose","render","Home","rightHead","burgerCont","bur","ger","onClick","burgerClick","isOpen","rightContent","About","Projects","listEnt","but","Research","tab","algo","textAlign","fig","rTwoFigOne","marginLeft","fontSize","App","React","useState","curActive","setCurActive","setMenuActive","container","topInfo","name","active","setActive","Boolean","window","location","hostname","match","ReactDOM","StrictMode","document","getElementById","navigator","serviceWorker","ready","then","registration","unregister","catch","error","console","message"],"mappings":"qGACAA,EAAOC,QAAU,CAAC,IAAM,iBAAiB,SAAW,sBAAsB,KAAO,kBAAkB,UAAY,uBAAuB,MAAQ,mBAAmB,QAAU,qBAAqB,IAAM,iBAAiB,IAAM,iBAAiB,UAAY,uBAAuB,WAAa,wBAAwB,IAAM,iBAAiB,IAAM,iBAAiB,aAAe,0BAA0B,WAAa,wBAAwB,IAAM,iBAAiB,UAAY,uBAAuB,UAAY,uBAAuB,IAAM,iBAAiB,UAAY,uBAAuB,YAAc,yBAAyB,MAAQ,mBAAmB,SAAW,sBAAsB,KAAO,oB,mBCDnrBD,EAAOC,QAAU,IAA0B,oC,6RC0CrCC,EAAO,SAAAC,GACX,OACE,kBAAC,IAAD,KACE,yBAAKC,UAAWC,IAAWC,IAAOC,WAChC,kBAAC,EAASJ,GACV,kBAAC,EAAUA,MAMbK,EAAO,SAAAL,GACX,OACE,yBAAKC,UAAWC,IAAWF,EAAMM,YAAcH,IAAOI,MAAOP,EAAMM,YAAcH,IAAOK,YACtF,yBAAKP,UAAWE,IAAOM,WACrB,yBAAKC,IAAI,+BAA+BC,IAAI,WAE9C,yBAAKV,UAAWE,IAAOS,aACrB,yBAAKX,UAAWE,IAAOU,UACrB,kBAAC,IAAD,CAAMC,GAAG,KACP,kDACA,kBAAC,IAAD,CAAOC,OAAO,EAAMC,KAAM,KAAK,gCAGnC,4BACE,wBAAIC,MAAO,CAACC,YAAa,sBACvB,yBAAKjB,UAAWE,IAAOgB,OACrB,kBAAC,IAAD,CAAML,GAAG,UAAT,aAEE,kBAAC,IAAD,CAAOE,KAAM,UAAU,wBAAIC,MAAO,CAACG,YAAa,cAItD,wBAAIH,MAAO,CAACC,YAAa,sBACvB,yBAAKjB,UAAWE,IAAOgB,OACrB,kBAAC,IAAD,CAAML,GAAG,aAAT,gBAEE,kBAAC,IAAD,CAAOE,KAAM,aAAa,wBAAIC,MAAO,CAACG,YAAa,cAIzD,wBAAIH,MAAO,CAACC,YAAa,sBACvB,yBAAKjB,UAAWE,IAAOgB,OACrB,kBAAC,IAAD,CAAML,GAAG,aAAT,gBAEI,kBAAC,IAAD,CAAOE,KAAM,aAAa,wBAAIC,MAAO,CAACG,YAAa,eAM7D,4BACE,4BACE,uBAAGC,KAAK,uCACN,0BAAMJ,MAAO,CAACC,YAAa,qBAAsBI,aAAc,uBAC7D,kBAAC,IAAD,CAAiBC,KAAMC,QAG3B,uBAAGH,KAAK,gCACN,0BAAMJ,MAAO,CAACC,YAAa,qBAAsBI,aAAc,uBAC7D,kBAAC,IAAD,CAAiBC,KAAME,QAG3B,uBAAGJ,KAAK,6BACN,0BAAMJ,MAAO,CAACC,YAAa,qBAAsBI,aAAc,uBAC7D,kBAAC,IAAD,CAAiBC,KAAMG,aAuDvC,IAAMC,EAASC,aA5Cf,SAAgBC,EAAYC,EAAgBC,GAC1C,IAAIC,GAAQ,EAEZH,EAAWI,MAAQ,WACjBJ,EAAWK,aAAa,IAAK,KAC7BL,EAAWM,SACXN,EAAWO,WAAW,GAAI,GAAI,IAC9BP,EAAWQ,OAAO,IAAI,IACtBR,EAAWS,UAAU,KAGvBT,EAAWU,aAAe,WACxBP,GAASA,GAGXH,EAAWW,KAAO,WAChB,IAAKR,EAAO,CACVH,EAAWY,OACXZ,EAAWa,UAAUb,EAAWc,MAAQ,EAAGd,EAAWe,OAAS,GAC/Df,EAAWgB,OAAOhB,EAAWiB,YAE7B,IAAMC,EAAmBC,SACvBnB,EAAWoB,IAAIpB,EAAWqB,OAAS,GAAI,EAAGrB,EAAWe,OAAQ,EAAG,KAE5DO,EAAStB,EAAWuB,OAASvB,EAAWc,MAAQ,EAChDU,EAAS,EAAIC,KAAKC,GAAMR,EAE9BlB,EAAW2B,aAEX,IAAK,IAAIC,EAAI,EAAGA,GAAKV,EAAkBU,IAAK,CAC1C,IAAMC,EAAIJ,KAAKK,IAAIN,EAAQI,GAAKN,EAC1BS,EAAIN,KAAKO,IAAIR,EAAQI,GAAKN,EAEhCtB,EAAWiC,aAAaL,EAAI,GAC5B5B,EAAWkC,OAAOL,EAAGE,GAGvB/B,EAAWmC,SAASnC,EAAWoC,OAE/BpC,EAAWqC,WAOXC,EAAQ,SAAAnE,GACZ,OACE,yBAAKC,UAAWC,IAAWF,EAAMM,YAAcH,IAAOiE,OAAQpE,EAAMM,YAAcH,IAAOkE,aACrF,kBAAC,IAAD,CAAOtD,OAAO,EAAMC,KAAM,KACxB,kBAAC,EAAShB,IAEZ,kBAAC,IAAD,CAAOgB,KAAM,SAAUsD,OAAQ,kBAAM,kBAAC,EAAUtE,MAChD,kBAAC,IAAD,CAAOgB,KAAM,YAAasD,OAAQ,kBAAM,kBAAC,EAAatE,MACtD,kBAAC,IAAD,CAAOgB,KAAM,YAAasD,OAAQ,kBAAM,kBAAC,EAAatE,QAKxDuE,EAAO,SAAAvE,GACX,OACE,6BACE,yBAAKC,UAAWE,IAAOqE,WACrB,yBAAKvE,UAAWE,IAAOsE,YACrB,yBAAKxE,UAAWE,IAAOuE,KACrB,kBAAC,IAAD,CAAM5D,GAAG,KAAI,sCAEf,yBAAKb,UAAWE,IAAOwE,IAAKC,QAAS5E,EAAM6E,aACvC,kBAAC,IAAD,CAAQC,OAAS9E,EAAMM,gBAI/B,yBAAKL,UAAWE,IAAO4E,cACvB,kBAACpD,EAAD,SAOAqD,EAAQ,SAAAhF,GACZ,OACE,6BACE,yBAAKC,UAAWE,IAAOqE,WACvB,yBAAKvE,UAAWE,IAAOsE,YACnB,yBAAKxE,UAAWE,IAAOuE,KACrB,kBAAC,IAAD,CAAM5D,GAAG,UAAS,uCAEpB,yBAAKb,UAAWE,IAAOwE,IAAKC,QAAS5E,EAAM6E,aACvC,kBAAC,IAAD,CAAQC,OAAS9E,EAAMM,eAN/B,KASA,yBAAKL,UAAWE,IAAO4E,cACvB,6BACA,+TAGA,2TASAE,EAAW,SAAAjF,GACf,OACE,6BACE,yBAAKC,UAAWE,IAAOqE,WACvB,yBAAKvE,UAAWE,IAAOsE,YACnB,yBAAKxE,UAAWE,IAAOuE,KACrB,kBAAC,IAAD,CAAM5D,GAAG,aAAY,0CAEvB,yBAAKb,UAAWE,IAAOwE,IAAKC,QAAS5E,EAAM6E,aACvC,kBAAC,IAAD,CAAQC,OAAS9E,EAAMM,gBAK/B,yBAAKL,UAAWE,IAAO4E,cACvB,yBAAK9E,UAAWE,IAAO+E,SACrB,kBAAC,IAAD,CAAMpE,GAAG,uBACP,8EACA,+BAIJ,kBAAC,IAAD,CAAOC,OAAO,EAAMC,KAAK,uBACvB,yBAAKf,UAAWE,IAAOgF,KACrB,uBAAG9D,KAAK,wCAAR,aACA,uBAAGA,KAAK,wCAAR,eAEF,6BACA,+CAIA,6BAAK,8BAEP,yBAAKpB,UAAWE,IAAO+E,SACrB,kBAAC,IAAD,CAAMpE,GAAG,uBACP,yEACA,+BAGJ,kBAAC,IAAD,CAAOC,OAAO,EAAMC,KAAK,uBACvB,yBAAKf,UAAWE,IAAOgF,KACrB,uBAAG9D,KAAK,wCAAR,aACA,uBAAGA,KAAK,wCAAR,eAEF,6BACA,gxIAeA,6BACA,iCAQF+D,EAAW,SAAApF,GACf,OACE,6BACE,yBAAKC,UAAWE,IAAOqE,WACrB,yBAAKvE,UAAWE,IAAOsE,YACrB,yBAAKxE,UAAWE,IAAOuE,KACrB,kBAAC,IAAD,CAAM5D,GAAG,aAAY,0CAEvB,yBAAKb,UAAWE,IAAOwE,IAAKC,QAAS5E,EAAM6E,aACvC,kBAAC,IAAD,CAAQC,OAAS9E,EAAMM,gBAI/B,yBAAKL,UAAWE,IAAO4E,cACrB,yBAAK9E,UAAWE,IAAO+E,SACrB,kBAAC,IAAD,CAAMpE,GAAG,wBACP,6GACA,uEACA,+BAGN,kBAAC,IAAD,CAAOC,OAAO,EAAMC,KAAK,wBACvB,yBAAKf,UAAWE,IAAOgF,KACrB,uBAAG9D,KAAK,uDAAR,aACA,uBAAGA,KAAK,sFAAR,eAEF,6BACA,wCACE,ogBAGF,4CACE,yrBAGA,kmBAGF,4CACE,ycAGA,qaAGA,g5BAGA,8oBAGA,iPAGA,uDACA,yBAAKpB,UAAWE,IAAOkF,IAAKpE,MAAO,CAAC0B,MAAO,QAC3C,+BACE,+BACE,4BACE,qCACA,4CACA,yDACA,iEAGJ,+BACE,4BACE,sEACA,gFACA,mCACA,oCAEF,4BACE,+DACA,wGACA,gFACA,oCAEF,4BACE,sEACA,oGACA,wFACA,kDAEF,4BACE,0DACA,uIACA,uEACA,iDAEF,4BACE,0DACA,0NACA,0JACA,6CAMR,yCACE,knBAGA,wCACA,wGAC4E,kBAAC,aAAD,KAAa,mBADzF,WACgI,kBAAC,aAAD,KAAa,UAD7I,gBACgL,kBAAC,aAAD,KAAa,UAD7L,iEACiR,kBAAC,aAAD,KAAa,SAD9R,yCACyV,kBAAC,aAAD,KAAa,yBADtW,OAC+Y,kBAAC,aAAD,KAAa,OAD5Z,KAGA,2BACE,kBAAC,aAAD,KAAa,KADf,mJAGA,iEACA,yvBAGA,qDACA,gRAGA,kEACA,47BAGA,kBAAC,YAAD,KAAY,2FACZ,2BACE,kBAAC,aAAD,KAAa,aADf,uEAC4G,kBAAC,aAAD,KAAa,KADzH,sCAC6K,kBAAC,aAAD,KAAa,KAD1L,uFAC+R,kBAAC,aAAD,KAAa,kBAD5S,iGACwa,kBAAC,aAAD,KAAa,SADrb,iEACygB,kBAAC,aAAD,KAAa,SADthB,yLACiuB,kBAAC,aAAD,KAAa,WAD9uB,qGACu2B,kBAAC,aAAD,KAAa,OAGp3B,8DACA,yBAAK1C,UAAWE,IAAOkF,IAAKpE,MAAO,CAAC0B,MAAO,QAC3C,+BACE,+BACE,4BACE,qCACA,gDACA,kDACA,qDAGJ,+BACE,4BACE,sEACA,+EACA,mCACA,oCAEF,4BACE,+DACA,wFACA,uEACA,oCAEF,4BACE,sEACA,qEACA,2DACA,oCAEF,4BACE,0DACA,uFACA,uEACA,wCAKR,uDACE,wCACE,+sBAGF,uCACE,mlBAGF,kDACE,gQAGA,mPAGF,uCACE,uKAIA,yBAAK1C,UAAWE,IAAOkF,IAAKpE,MAAO,CAAC0B,MAAO,QACzC,+BACI,+BACI,4BACI,qCACA,uCACA,uCACA,uCACA,uCAGR,+BACI,4BACE,8EAGA,qCAGA,qCAGA,qCAGA,sCAIF,4BACE,0FAGA,qCAGA,qCAGA,qCAGA,sCAIF,4BACE,mEAGA,qCAGA,qCAGA,qCAGA,sCAIF,4BACE,sEAGA,qCAGA,qCAGA,qCAGA,sCAIF,4BACE,kEAGA,qCAGA,qCAGA,qCAGA,yCAOZ,8DACA,yBAAK1C,UAAWE,IAAOkF,IAAKpE,MAAO,CAAC0B,MAAO,QACzC,+BACI,+BACE,4BACE,wBAAIA,MAAM,QAAV,aAGJ,+BACE,4BACE,6tEAGJ,+BACE,4BACE,wBAAIA,MAAM,QAAV,mBAGJ,+BACE,4BACE,6UAGJ,+BACE,4BACE,wBAAIA,MAAM,QAAV,oDAGJ,+BACE,4BACE,iRAGJ,+BACE,4BACE,wBAAIA,MAAM,QAAV,iDAGJ,+BACE,4BACE,2SAGJ,+BACE,4BACE,wBAAIA,MAAM,QAAV,oDAGJ,+BACE,4BACE,6WAGJ,+BACE,4BACE,wBAAIA,MAAM,QAAV,gDAGJ,+BACE,4BACE,4RAIR,6BACA,+BACI,+BACE,4BACE,wBAAIA,MAAM,QAAV,aAGJ,+BACE,4BACE,ymEAGJ,+BACE,4BACE,wBAAIA,MAAM,QAAV,mBAGJ,+BACE,4BACE,iYAGJ,+BACE,4BACE,wBAAIA,MAAM,QAAV,oDAGJ,+BACE,4BACE,iQAGJ,+BACE,4BACE,wBAAIA,MAAM,QAAV,iDAGJ,+BACE,4BACE,uaAGJ,+BACE,4BACE,wBAAIA,MAAM,QAAV,oDAGJ,+BACE,4BACE,qRAGJ,+BACE,4BACE,wBAAIA,MAAM,QAAV,gDAGJ,+BACE,4BACE,mWAMZ,wCACE,25BAGA,u2BAGA,4JAGA,knBAGJ,0DACA,k3BAGA,0CACA,4NACA,2MACA,gPACA,4MACA,wPACA,mLACA,wLACA,4HACA,mLACA,kKACA,6BACA,8BAGF,yBAAK1C,UAAWE,IAAO+E,SACrB,kBAAC,IAAD,CAAMpE,GAAG,wBACP,4GACA,6DACA,+BAGJ,kBAAC,IAAD,CAAOE,KAAK,wBACV,yBAAKf,UAAWE,IAAOgF,KACrB,uBAAG9D,KAAK,8CAAR,aACA,uBAAGA,KAAK,sFAAR,eAEF,6BACA,wCACA,qkCAGA,4CACA,4cAGA,upBAGA,0KAC8I,kBAAC,aAAD,KAAa,qBAD3J,cACuM,kBAAC,aAAD,KAAa,sBADpN,QAC2P,kBAAC,aAAD,KAAa,qBADxQ,iBACuT,kBAAC,aAAD,KAAa,OADpU,2DAC+Y,kBAAC,aAAD,KAAa,aAD5Z,sDACwe,kBAAC,aAAD,KAAa,cADrf,6BACyiB,kBAAC,aAAD,KAAa,MADtjB,4CACinB,kBAAC,aAAD,KAAa,aAD9nB,qCACyrB,kBAAC,aAAD,KAAa,kBADtsB,qCACswB,kBAAC,aAAD,KAAa,kBADnxB,iEAC+2B,kBAAC,aAAD,KAAa,KAD53B,4IAGA,yBAAKpB,UAAWE,IAAOmF,MACrB,+BACE,+BACE,wBAAIrE,MAAO,CAACsE,UAAW,WAAvB,SACQ,kBAAC,aAAD,KAAa,WADrB,eAIF,+BACE,4BACE,wBAAItE,MAAO,CAACC,YAAa,SACzB,qCADA,kBAC4B,kBAAC,aAAD,KAAa,WAG3C,4BACE,wBAAID,MAAO,CAACC,YAAa,SAAzB,cAA6C,kBAAC,aAAD,KAAa,mDAE5D,4BACE,wBAAID,MAAO,CAACC,YAAa,SAAS,mCAAlC,IAA8C,kBAAC,aAAD,KAAa,qBAAiC,qCAE9F,4BACE,wBAAID,MAAO,CAACC,YAAa,SAAzB,oBAAmD,kBAAC,aAAD,KAAa,gCAElE,4BACE,wBAAID,MAAO,CAACC,YAAa,SAAS,mCAAlC,IAA8C,kBAAC,aAAD,KAAa,qBAAiC,qCAE9F,4BACE,wBAAID,MAAO,CAACC,YAAa,UAAzB,OAAuC,kBAAC,aAAD,KAAa,oEAEtD,4BACE,wBAAID,MAAO,CAACC,YAAa,SAAS,wCAEpC,4BACE,wBAAID,MAAO,CAACC,YAAa,SAAS,mCAAlC,IAA8C,kBAAC,aAAD,KAAa,qBAAiC,qCAE9F,4BACE,wBAAID,MAAO,CAACC,YAAa,UAAzB,OAAuC,kBAAC,aAAD,KAAa,wDAEtD,4BACE,wBAAID,MAAO,CAACC,YAAa,SAAS,wCAEpC,4BACE,wBAAID,MAAO,CAACC,YAAa,SAAzB,UAAyC,kBAAC,aAAD,KAAa,oEAExD,4BACE,wBAAID,MAAO,CAACC,YAAa,SAAS,2CAK1C,gLAGA,yBAAKjB,UAAWE,IAAOmF,MACrB,+BACE,+BACE,wBAAIrE,MAAO,CAACsE,UAAW,WAAvB,UACS,kBAAC,aAAD,KAAa,WADtB,eAIF,+BACE,4BACE,wBAAItE,MAAO,CAACC,YAAa,SACzB,qCADA,kBAC4B,kBAAC,aAAD,KAAa,WAG3C,4BACE,wBAAID,MAAO,CAACC,YAAa,SAAzB,cAA6C,kBAAC,aAAD,KAAa,oDAE5D,4BACE,wBAAID,MAAO,CAACC,YAAa,SAAS,mCAAlC,IAA8C,kBAAC,aAAD,KAAa,qBAAiC,qCAE9F,4BACE,wBAAID,MAAO,CAACC,YAAa,SAAzB,oBAAmD,kBAAC,aAAD,KAAa,gCAElE,4BACE,wBAAID,MAAO,CAACC,YAAa,SAAzB,cAA6C,kBAAC,aAAD,KAAa,0CAE5D,4BACE,wBAAID,MAAO,CAACC,YAAa,SAAS,mCAAlC,IAA8C,kBAAC,aAAD,KAAa,qBAAiC,qCAE9F,4BACE,wBAAID,MAAO,CAACC,YAAa,UAAzB,OAAuC,kBAAC,aAAD,KAAa,0EAEtD,4BACE,wBAAID,MAAO,CAACC,YAAa,UAAzB,OAAuC,kBAAC,aAAD,KAAa,sEAApD,MAEF,4BACE,wBAAID,MAAO,CAACC,YAAa,UAAzB,6BAA6D,kBAAC,aAAD,KAAa,YAE5E,4BACE,wBAAID,MAAO,CAACC,YAAa,UAAU,mCAAnC,QAAmD,kBAAC,aAAD,KAAa,gBAA4B,qCAE9F,4BACE,wBAAID,MAAO,CAACC,YAAa,UAAzB,OAAuC,kBAAC,aAAD,KAAa,8DAEtD,4BACE,wBAAID,MAAO,CAACC,YAAa,UAAU,wCAErC,4BACE,wBAAID,MAAO,CAACC,YAAa,SAAS,wCAEpC,4BACE,wBAAID,MAAO,CAACC,YAAa,SAAS,2CAK1C,kDACA,wKAGA,ycAGA,2oBAGA,yBAAKjB,UAAWE,IAAOqF,KACrB,yBAAK9E,IAAK+E,MACV,uCAAY,sLAId,oDACE,kkBAGA,yBAAKxF,UAAWE,IAAOkF,IAAKpE,MAAO,CAAC0B,MAAO,QAC3C,2BAAO1B,MAAO,CAAC0B,MAAM,MAAO+C,WAAY,OAAQtE,YAAY,SAC1D,+BACE,4BACE,yCACA,0CACA,oCACA,gDAGJ,+BACE,4BACE,mCACA,kCACA,oCACA,oCAEF,4BACE,mCACA,kCACA,oCACA,oCAEF,4BACE,uCACA,kCACA,oCACA,qCAEF,4BACE,oCACA,kCACA,oCACA,qCAEF,4BACE,mCACA,mCACA,oCACA,oCAEF,4BACE,mCACA,mCACA,oCACA,oCAEF,4BACE,uCACA,mCACA,oCACA,oCAEF,4BACE,oCACA,mCACA,oCACA,sCAIN,uBAAGH,MAAO,CAAC0E,SAAU,SAArB,YAAuC,qHAGvC,0ZAGA,yBAAK1F,UAAWE,IAAOkF,IAAKpE,MAAO,CAAC0B,MAAO,QAC3C,2BAAO1B,MAAO,CAAC0B,MAAM,MAAO+C,WAAY,OAAQtE,YAAY,SAC1D,+BACE,4BACE,yCACA,mCACA,qCAGJ,+BACE,4BACE,mCACA,qCACA,sCAEF,4BACE,mCACA,qCACA,sCAEF,4BACE,uCACA,qCACA,sCAEF,4BACE,oCACA,qCACA,wCAKN,uBAAGH,MAAO,CAAC0E,SAAU,SAArB,aAAwC,4LAExC,wvBAGF,0CACA,4YAGA,6YAGA,0CACE,oKACF,6DACA,oRACA,2BAAG,6CAAqB,8DAAxB,IAA+D,kBAAC,aAAD,KAAa,6BAAyC,oDAA4B,kBAAC,aAAD,KAAa,qBAAiC,iFAAyD,kBAAC,aAAD,KAAa,0BAAsC,qCAA3S,IAAyT,kBAAC,aAAD,KAAa,oDAAgE,6CAAqB,kBAAC,aAAD,KAAa,0BAAsC,8EAC9c,kBAAC,YAAD,KAAY,uKACZ,2BAAG,qCAAa,kBAAC,aAAD,KAAa,sBAA7B,IAAgE,6CAAqB,kBAAC,aAAD,KAAa,iBAA6B,oCAAY,kBAAC,aAAD,KAAa,sBAAkC,mCAAW,kBAAC,aAAD,KAAa,sBAAlN,KACA,2BAAG,qCAAH,qCACA,2BAAG,6CAAH,IAAyB,wCAAgB,kBAAC,aAAD,KAAa,eAA2B,8CAAsB,kBAAC,aAAD,KAAa,oCAAgD,sGAA8E,kBAAC,aAAD,KAAa,oBAAgC,qDAA6B,kBAAC,aAAD,aAA6B,qCAAa,kBAAC,aAAD,KAAa,6DACnX,2BAAG,sCAAH,kDACA,2BAAG,6CAAqB,qEAA6C,kBAAC,aAAD,KAAa,oBAAgC,uEAA+C,kBAAC,aAAD,KAAa,qBAAiC,sEAA8C,kBAAC,aAAD,KAAa,QAAoB,oFAA4D,kBAAC,aAAD,KAAa,mCAA+C,0DAAkC,kBAAC,aAAD,KAAa,cAA0B,uGAC/d,kBAAC,YAAD,KAAY,qDACZ,kBAAC,YAAD,KAAY,6IACZ,kBAAC,YAAD,KAAY,yDACZ,kBAAC,YAAD,KAAY,sGACZ,2BAAG,oCAAY,kBAAC,aAAD,KAAa,wDAC5B,2BAAG,sCAAH,iCACA,kBAAC,YAAD,KAAY,mDACZ,kBAAC,YAAD,KAAY,sKACZ,mFACA,kBAAC,YAAD,KAAY,yFACZ,kBAAC,YAAD,KAAY,8GACZ,kBAAC,YAAD,KAAY,0EACZ,uDAA4B,kBAAC,aAAD,KAAa,yCAAzC,uDAAkJ,kBAAC,aAAD,KAAa,yCAA/J,4CAA6P,kBAAC,aAAD,KAAa,8BAA1Q,cACA,kBAAC,YAAD,KAAY,mHACZ,kBAAC,YAAD,KAAY,gEACZ,sEAA2C,kBAAC,aAAD,KAAa,uCAAxD,mBACA,kBAAC,YAAD,KAAY,8JACZ,4CACA,kBAAC,YAAD,KAAY,sJACZ,kBAAC,YAAD,KAAY,iIACZ,0DAA+B,kBAAC,aAAD,KAAa,UAA5C,QAAuE,kBAAC,aAAD,KAAa,WAApF,2DACA,4CACA,uNACA,oJACA,+OACA,6BACA,iCAMOC,EA7gCH,WACR,IADc,EAYoBC,IAAMC,SAAS,SAZnC,mBAYPC,EAZO,KAYIC,EAZJ,OAasBH,IAAMC,UAAS,GAbrC,mBAaPxF,EAbO,KAaK2F,EAbL,KAoBhB,OACE,yBAAKhG,UAAWC,IAAWC,IAAO+F,YAC9B,kBAAC,EAAD,CAAMC,QArBG,CACT,CACIC,KAAM,QACN/E,KAAM,IAEV,CACI+E,KAAM,WACN/E,KAAM,KAcWgF,OAAQN,EAAWO,UAAWN,EAAc1F,WAAYA,EAAYuE,YAPzE,WAClBoB,GAAe3F,QCnBDiG,QACW,cAA7BC,OAAOC,SAASC,UAEe,UAA7BF,OAAOC,SAASC,UAEhBF,OAAOC,SAASC,SAASC,MACvB,2DCZNC,IAAStC,OACP,kBAAC,IAAMuC,WAAP,KACE,kBAAC,EAAD,OAEFC,SAASC,eAAe,SDyHpB,kBAAmBC,WACrBA,UAAUC,cAAcC,MACrBC,MAAK,SAAAC,GACJA,EAAaC,gBAEdC,OAAM,SAAAC,GACLC,QAAQD,MAAMA,EAAME,c","file":"static/js/main.528b750c.chunk.js","sourcesContent":["// extracted by mini-css-extract-plugin\nmodule.exports = {\"App\":\"App_App__15LM-\",\"mainBody\":\"App_mainBody__188oX\",\"left\":\"App_left__1ks1Q\",\"leftClose\":\"App_leftClose__20-Ts\",\"right\":\"App_right__1V0tl\",\"listEnt\":\"App_listEnt__1TcTN\",\"tab\":\"App_tab__3-RO0\",\"fig\":\"App_fig__GY9HQ\",\"rightHead\":\"App_rightHead__xMDKF\",\"burgerCont\":\"App_burgerCont__qXB5B\",\"bur\":\"App_bur__3skdA\",\"ger\":\"App_ger__3wuke\",\"rightContent\":\"App_rightContent__i6iar\",\"rightClose\":\"App_rightClose__3sWT9\",\"but\":\"App_but__1a1lG\",\"rightHome\":\"App_rightHome__2lqZd\",\"container\":\"App_container__eSJ6i\",\"top\":\"App_top__3N23O\",\"leftImage\":\"App_leftImage__2Bar7\",\"leftContent\":\"App_leftContent__3x1tj\",\"entry\":\"App_entry__bJLBI\",\"entryTop\":\"App_entryTop___deda\",\"algo\":\"App_algo__2-d3V\"};","module.exports = __webpack_public_path__ + \"static/media/FigOne.5b7a72f9.png\";","import React from 'react';\nimport 'katex/dist/katex.min.css';\nimport Burger from \"@animated-burgers/burger-rotate\"; \nimport \"@animated-burgers/burger-rotate/dist/styles.css\";\nimport { InlineMath, BlockMath } from 'react-katex';\nimport { BrowserRouter as Router, Link, Route } from 'react-router-dom';\nimport styles from './App.module.scss';\nimport classNames from 'classnames';\nimport { FontAwesomeIcon } from '@fortawesome/react-fontawesome';\nimport { faEnvelope, faSwatchbook, faCode, faScroll, faArrowRight } from '@fortawesome/free-solid-svg-icons';\nimport { faTwitter, faGithub } from '@fortawesome/free-brands-svg-icons';\nimport rTwoFigOne from './assets/FigOne.png';\n\nimport { createP5Sketch } from \"generative-art-tools\";\n\nconst App = () => {\n    const menu = [\n        {\n            name: 'Intro',\n            href: '',\n        },\n        {\n            name: 'Projects',\n            href: '',\n        }\n        ]\n\n    const [curActive, setCurActive] = React.useState('Intro');\n    const [menuActive, setMenuActive] = React.useState(true);\n\n    const burgerClick = () => {\n      setMenuActive(!menuActive)\n    };\n\n\n  return (\n    <div className={classNames(styles.container)}>\n        <Body topInfo={menu} active={curActive} setActive={setCurActive} menuActive={menuActive} burgerClick={burgerClick}/>\n    </div>\n  );\n}\n\nconst Body = props => {\n  return (\n    <Router>\n      <div className={classNames(styles.mainBody)}>\n        <Left {...props}/>\n        <Right {...props}/>\n      </div>\n      </Router>\n  );\n};\n\nconst Left = props => {\n  return (\n    <div className={classNames(props.menuActive && styles.left, !props.menuActive && styles.leftClose)}>\n      <div className={styles.leftImage}>\n        <img src=\"https://i.gifer.com/NYRT.gif\" alt=\"intro\"></img>\n      </div>\n      <div className={styles.leftContent }>\n        <div className={styles.entryTop}>\n          <Link to='/'>\n            <h4>Jaime Campos Salas</h4>\n            <Route exact={true} path={'/'}><hr/></Route>\n          </Link>\n        </div>\n        <ul>\n          <li style={{paddingLeft: \"calc(16.6% - 12px\"}}>\n            <div className={styles.entry}>\n              <Link to='/about'>\n                About &raquo; \n                <Route path={'/about'}><hr style={{marginRight: \"90px\"}}/></Route>\n              </Link>\n            </div>\n          </li>\n          <li style={{paddingLeft: \"calc(16.6% - 12px\"}}>\n            <div className={styles.entry}>\n              <Link to='/projects'>\n                Projects &raquo;\n                <Route path={'/projects'}><hr style={{marginRight: \"75px\"}}/></Route>\n              </Link>\n            </div>\n          </li>\n          <li style={{paddingLeft: \"calc(16.6% - 12px\"}}>\n            <div className={styles.entry}>\n              <Link to='/research'>\n                  Research &raquo;\n                  <Route path={'/research'}><hr style={{marginRight: \"72px\"}}/></Route>        \n              </Link>\n            </div>\n            \n          </li>\n        </ul>\n        <ul>\n          <li>\n            <a href=\"mailto:jaime.campos.salas@gmail.com\">\n              <span style={{paddingLeft: \"calc(16.6% - 12px)\", paddingRight: \"calc(16.6% - 12px)\"}}>\n                <FontAwesomeIcon icon={faEnvelope} />\n              </span>\n            </a>\n            <a href=\"https://twitter.com/xaimehcs\">\n              <span style={{paddingLeft: \"calc(16.6% - 12px)\", paddingRight: \"calc(16.6% - 12px)\"}}>\n                <FontAwesomeIcon icon={faTwitter} />\n              </span>\n            </a>\n            <a href=\"https://github.com/jcoeus\">\n              <span style={{paddingLeft: \"calc(16.6% - 12px)\", paddingRight: \"calc(16.6% - 12px)\"}}>\n                <FontAwesomeIcon icon={faGithub} />\n              </span>\n            </a>\n          </li>\n        </ul>\n      </div>\n    </div>\n  );\n\n};\n\nfunction sketch(p5Instance, componentProps, wrapperElement) {\n  let pause = true;\n\n  p5Instance.setup = function() {\n    p5Instance.createCanvas(630, 500);\n    p5Instance.noFill();\n    p5Instance.background(36, 45, 95);\n    p5Instance.stroke(255,15);\n    p5Instance.frameRate(30);\n  };\n\n  p5Instance.mousePressed = function() {\n    pause = !pause;\n  };\n\n  p5Instance.draw = function() {\n    if (!pause) {\n      p5Instance.push();\n      p5Instance.translate(p5Instance.width / 2, p5Instance.height / 2);\n      p5Instance.rotate(p5Instance.frameCount);\n\n      const circleResolution = parseInt(\n        p5Instance.map(p5Instance.mouseY + 50, 0, p5Instance.height, 2, 10)\n      );\n      const radius = p5Instance.mouseX - p5Instance.width / 2;\n      const angle = (2 * Math.PI) / circleResolution;\n\n      p5Instance.beginShape();\n\n      for (let i = 0; i <= circleResolution; i++) {\n        const x = Math.cos(angle * i) * radius;\n        const y = Math.sin(angle * i) * radius;\n\n        p5Instance.strokeWeight(i / 2);\n        p5Instance.vertex(x, y);\n      }\n\n      p5Instance.endShape(p5Instance.CLOSE);\n\n      p5Instance.pop();\n    }\n  };\n}\n\nconst Sketch = createP5Sketch(sketch);\n\nconst Right = props => {\n  return (\n    <div className={classNames(props.menuActive && styles.right, !props.menuActive && styles.rightClose)}>\n        <Route exact={true} path={'/'}> \n          <Home {...props}/>\n        </Route>\n        <Route path={'/about'} render={() => <About {...props}/>}/>\n        <Route path={'/projects'} render={() => <Projects {...props}/>}/>\n        <Route path={'/research'} render={() => <Research {...props}/>}/>\n    </div>\n  );\n};\n\nconst Home = props => {\n  return (\n    <div>\n      <div className={styles.rightHead}>\n        <div className={styles.burgerCont}>\n          <div className={styles.bur}>\n            <Link to=\"/\"><h2>Home</h2></Link>\n          </div>\n          <div className={styles.ger} onClick={props.burgerClick}>\n              <Burger isOpen={ props.menuActive }/>\n          </div>  \n        </div>\n      </div>\n      <div className={styles.rightContent}>\n      <Sketch></Sketch>\n      </div>\n    </div>\n\n  );\n};\n\nconst About = props => {\n  return (\n    <div >\n      <div className={styles.rightHead}>\n      <div className={styles.burgerCont}>\n          <div className={styles.bur}>\n            <Link to=\"/about\"><h2>About</h2></Link>\n          </div>\n          <div className={styles.ger} onClick={props.burgerClick}>\n              <Burger isOpen={ props.menuActive }/>\n          </div>  \n        </div> </div>\n      <div className={styles.rightContent}>\n      <br/>\n      <p>\n        My name is Jaime (pronounced like xai-meh). I like working with machines and, sometimes, machines like working with me too. If time and computing permits, I occasionally work on NLP/U and Deep Learning research. Though not particularly good at it, I also enjoy writing and generative art.\n      </p>\n      <p>\n        I graduated Columbia University where I studied CS and lots of math. During my time there, I helped build a suspension bridge and a water well for an underserved community in Morocco. I also helped kickstart the Medical Informatics Society where I hosted machine learning lectures.\n      </p>\n      \n      </div>\n    </div>\n  );\n};\n\nconst Projects = props => {\n  return (\n    <div>\n      <div className={styles.rightHead}>\n      <div className={styles.burgerCont}>\n          <div className={styles.bur}>\n            <Link to=\"/projects\"><h2>Projects</h2></Link>\n          </div>\n          <div className={styles.ger} onClick={props.burgerClick}>\n              <Burger isOpen={ props.menuActive }/>\n          </div>  \n        </div>\n       \n      </div>\n      <div className={styles.rightContent}>\n      <div className={styles.listEnt}>\n        <Link to='/projects/project-1'>\n          <h3>Columbia Engineering Outreach Program Database</h3>\n          <hr/>\n        </Link>\n      </div>\n      \n      <Route exact={true} path='/projects/project-1'>\n        <div className={styles.but}>\n          <a href=\"https://arxiv.org/pdf/2006.09549.pdf\">code &raquo;</a>\n          <a href=\"https://arxiv.org/pdf/2006.09549.pdf\">paper &raquo;</a>\n        </div>\n        <hr/>\n        <p>\n          Some description\n        </p> \n\n        <hr/><hr/>\n      </Route>\n      <div className={styles.listEnt}>\n        <Link to='/projects/project-2'>\n          <h3>Compiler Module for Dead Code Elimination</h3>\n          <hr/>\n        </Link>\n      </div>\n      <Route exact={true} path='/projects/project-2'>\n        <div className={styles.but}>\n          <a href=\"https://arxiv.org/pdf/2006.09549.pdf\">code &raquo;</a>\n          <a href=\"https://arxiv.org/pdf/2006.09549.pdf\">paper &raquo;</a>\n        </div>\n        <hr/>\n        <p>\n          Nam ullamcorper, diam a rhoncus rhoncus, orci urna vehicula nisi, eu sagittis sapien metus in lacus. Ut quis tempus diam. Etiam hendrerit suscipit lobortis. Donec in ullamcorper nisi, at hendrerit odio. Pellentesque lobortis eleifend mattis. Etiam in dui sed ex posuere auctor malesuada ut nibh. Suspendisse consequat nibh metus, at tincidunt neque pretium in. Curabitur egestas nibh id lacus laoreet, at suscipit justo auctor. Sed ac auctor leo. Cras ac molestie leo, molestie vehicula sapien. Aenean sodales condimentum ex, at vulputate risus tempor in. Etiam ut elit metus. Curabitur eu sem nec ipsum scelerisque aliquet a a tortor.\n\n          Nulla et tempus justo. Quisque congue vehicula magna sed vehicula. Nam aliquet lacus ac pretium tristique. Nam sit amet bibendum urna. Sed et ex lacinia sapien lobortis placerat. Quisque odio odio, semper quis erat vitae, volutpat tincidunt sem. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia curae; Nam lorem ligula, vulputate quis suscipit in, ornare in tortor. Integer tincidunt volutpat scelerisque. Aliquam dignissim blandit aliquet. Duis magna augue, rhoncus quis sollicitudin vitae, mattis vitae lectus.\n\n          Phasellus mi purus, interdum eget placerat nec, rutrum non enim. Pellentesque pretium dictum aliquet. Vivamus placerat felis tortor, vitae tempor augue aliquam eu. Aenean eleifend, nunc eu lacinia ullamcorper, erat diam cursus urna, scelerisque posuere quam nibh sed quam. Donec pharetra augue vel eros bibendum, convallis auctor sapien convallis. Proin nibh dui, suscipit sit amet lacus ac, interdum bibendum est. Maecenas egestas tincidunt tellus vel accumsan. Cras consequat finibus odio, ac imperdiet velit vestibulum vel. Sed vehicula, odio sit amet bibendum sollicitudin, magna nunc rutrum diam, vel eleifend felis erat id neque. Integer efficitur dolor purus, et aliquam justo vehicula quis. Morbi iaculis viverra nisl quis aliquam. In suscipit a urna quis hendrerit. Curabitur vitae lacus elementum, ultricies nisl id, bibendum nibh. Nullam fermentum iaculis consequat. Quisque tempor fringilla facilisis.\n\n          Duis et orci semper, tincidunt massa vel, rhoncus dui. Phasellus nec tortor at quam ultrices tempus nec at metus. Ut nec ligula urna. In hac habitasse platea dictumst. Sed sagittis accumsan interdum. Morbi euismod condimentum justo et malesuada. Quisque feugiat est ac est sagittis, sed faucibus ex tincidunt. Integer placerat at lectus id consequat. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Donec molestie nunc fringilla condimentum porta. In eu turpis vitae augue molestie volutpat eget ut augue. Ut finibus sodales sem, eget pulvinar risus ultricies ac. Phasellus blandit quis nibh eget ullamcorper. Proin purus sem, sagittis convallis fringilla in, venenatis nec nisi.\n\n          Praesent non hendrerit tortor. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Praesent ligula ex, scelerisque non faucibus sed, tincidunt nec diam. Proin posuere ipsum sem, nec dignissim nibh pretium sed. Nam volutpat a nunc a tristique. Donec at feugiat tortor. Sed eleifend molestie porta. Donec pellentesque lectus quis tellus consectetur tincidunt. Nam blandit nunc vitae suscipit malesuada. Nulla dolor justo, lacinia ac erat eget, venenatis hendrerit lorem. Curabitur sed tempus lacus. Vivamus rutrum maximus auctor. Sed maximus turpis id nisl congue, at vestibulum libero luctus. Donec elementum blandit nunc, et euismod dolor venenatis non. Maecenas ultrices congue metus, in auctor ligula mollis sit amet.\n\n          Integer sollicitudin odio vitae varius vulputate. Quisque vitae placerat diam, vitae egestas justo. Donec ultricies lorem in consequat tincidunt. Pellentesque eu turpis posuere, ultricies purus vel, sollicitudin mauris. Phasellus pretium justo leo, et pulvinar lorem venenatis non. Praesent hendrerit iaculis nibh, eget commodo magna aliquet ac. Sed eget placerat ligula. In hac habitasse platea dictumst. Sed vitae pharetra leo.\n\n          Mauris pulvinar volutpat massa id aliquet. Donec id scelerisque elit. Suspendisse efficitur pretium nulla, eu egestas lorem blandit quis. Pellentesque non congue ipsum. Nunc pharetra enim id nunc dignissim sollicitudin. Nunc fringilla porttitor tellus. Sed a ante enim. Quisque sed purus congue, gravida nisl a, pretium dolor.\n        </p>\n        <hr/>\n        <hr/>\n      </Route>\n      </div>\n    </div>\n \n  )\n};\n\nconst Research = props =>  {\n  return (\n    <div>\n      <div className={styles.rightHead}>\n        <div className={styles.burgerCont}>\n          <div className={styles.bur}>\n            <Link to=\"/research\"><h2>Research</h2></Link>\n          </div>\n          <div className={styles.ger} onClick={props.burgerClick}>\n              <Burger isOpen={ props.menuActive }/>\n          </div>  \n        </div>\n      </div>\n      <div className={styles.rightContent}>\n        <div className={styles.listEnt}>\n          <Link to='/research/research-1'>\n            <h3>Text Summarization with Modern Word Embeddings and Pointer-Generator Networks</h3>\n            <p>with Andres Talero and Abdullah Siddique</p>\n            <hr/>\n          </Link>\n        </div>\n      <Route exact={true} path='/research/research-1'>\n        <div className={styles.but}>\n          <a href=\"https://github.com/atalero/summarization_embeddings\">code &raquo;</a>\n          <a href=\"https://drive.google.com/file/d/1DdHbIZNw4FR266i7wmIm9tgZXpbmYzRT/view?usp=sharing\">paper &raquo;</a>\n        </div>\n        <hr/>\n        <h4>Abstract</h4>\n          <p>\n            We conduct a performance overview of novel word embedding developments over the past two decades. We apply transfer learning to a text summarization neural network by administering different word embedding types to the network, which uses a combination of extractive and abstractive methods. We describe the differences in training and implementation for different word embedding types and produce a quantitative and qualitative analysis that compares the results produced by each one.\n          </p>\n        <h4>Introduction</h4>\n          <p>\n            Word embeddings in neural networks are often times learned as part of the NLP task, where there is a one-to-one correspondence between words and their respective low-dimensional vector representations. This is the case in the text summarization model by See et al. (2017). While this model adds innovations that allow it to effectively employ abstractive and extractive techniques for text summarization, it still uses the aforementioned embedding matrix mechanism. We aim to improve this part of the network via transfer learning. We use state-of-the-art embeddings and embedding models that are known to have improved other NLP tasks to improve text summarization.\n          </p>\n          <p>\n            The models we use are Embeddings from Language Models (ELMo; Peters et al.,2018), Global Vectors for Word Representation (GloVe; Pennington et. al, 2014), and Efficient Estimation of Word Representations in Vector Space (Word2Vec; Mikolov et. al, 2013). The baseline model (See et al. 2017) uses ROUGE scores as a metric. We show that introducing new word embeddings improves the quality of test summaries with respect to the baseline model. Finally, we conduct an evaluation of our results, in which we compare the ROUGE scores and some sample summaries for each embedding type.\n          </p>\n        <h4>Related Work</h4>\n          <p>\n            See et al.’s (2017) model includes two main mechanisms on top of a basic encoder (bidirectional RNN with LSTM cells) - decoder (unidirectional RNN with LSTM cells) model. The mechanisms are coverage, which uses a penalty function to avoid excessive copying of words from the source text, and a pointer-generator network, which allows the model to both copy words from the source text and generate words from a vocabulary.\n          </p>\n          <p>\n            Pennington et. al (2014) provide a high-level explanation of their GloVe; their bi-linear re- gression model \"combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods.\" They add that it \"efficiently leverages statistical information by training only on the nonzero elements in a word-word co-occurrence matrix.\"\n          </p>\n          <p>\n            In their Word2Vec model, Mikolov et.al (2013) propose two simple methods for learning efficient and accurate vector representations of words. The first technique is continuous bag-of-words (CBOW) where the word is predicted using a window of surrounding words as the input (order of the surrounding words doesn’t matter). The second technique is continuous skip-gram where a word is the input and the surrounding words are predicted. In our project we used the skip-gram method. The idea behind skip-gram is to learn efficient vector representations of words and specifically learn similar words. The intuition behind this is that if two words have similar contexts then they should have similar representations as well. So instead of predicting the next word, the model predicts whether a pair of words is relevant or not an if they are then they have similar representaions.\n          </p>\n          <p>\n            ELMo (Peters et. al., 2018) stands out from other word vectorization algorithms for two reasons. Firstly, the input to the model is not a single word but a whole sentence. In this way, the model will capture the context the word is used in. This helps to resolve issues of ambiguity, like in the case of homographs. Secondly, the authors provide an architecture that may be used to solve a variety of NLP problems, making ELMo suitable for transfer learning. The vector representations for words for any given sentence are only partially learned, allowing for additional parameters to be tuned for a specific learning task.\n          </p>\n          <p>\n            Lastly, Deep Bidirectional Transformers for Language Understanding (BERT; Devlin et al., 2018) uses sentence context (like ELMo). Additionally, it makes use of transformers as opposed to traditional RNN’s.\n          </p>\n          <h5>Summary of Related Work</h5>\n          <div className={styles.tab} style={{width: \"80%\"}}>\n          <table >\n            <thead>\n              <tr>\n                <th>Model</th>\n                <th>Architecture</th>\n                <th>Pretrained Vector Dataset</th>\n                <th>Pretrained Dimensions Available</th>\n              </tr>\n            </thead>\n            <tbody>\n              <tr>\n                <td>Embedding Matrix (Bengio et al., 2003)</td>\n                <td>Embedding matrix learned as part of some network</td>\n                <td>N/A</td>\n                <td>N/A</td>\n              </tr>\n              <tr>\n                <td>Skipgram (Mikolov et al., 2003)</td>\n                <td>Words predicted within a certain range before and after the current word</td>\n                <td>Google New corpus (trained on 100 million words)</td>\n                <td>300</td>\n              </tr>\n              <tr>\n                <td>GloVe (GloVe; Pennington et al., 2014)</td>\n                <td>Trained on co-occurrence matrix with fixed window size (for context)</td>\n                <td>Several: Wikipedia, Gigaword 5, Common Crawl and Twitter</td>\n                <td>50, 100, 200, 300</td>\n              </tr>\n              <tr>\n                <td>ELMo (Peters et al., 2018)</td>\n                <td>Forward and backward language models consisting of layers LSTM's with a layer of character convolutions</td>\n                <td>1 billion Word Language Model Benchmark</td>\n                <td>1024, 2048, 4096</td>\n              </tr>\n              <tr>\n                <td>BERT (Devlin et al., 2018)</td>\n                <td>Transformers: \"a model archicture eschewing recurrence and instead relying entirel on an attention mechanism to draw global dependencies between input and output.\" (Vaswani et al., 2017)</td>\n                <td>Origianlly on BookCorpus (800 million words) and English Wikipedia (2,500 million words) - multilingual BERT now available</td>\n                <td>768, 1024</td>\n              </tr>\n            </tbody>\n          </table>\n          </div>\n          \n        <h4>Our Model</h4>\n          <p>\n            See et al.’s (2017) architecture includes a simple embedding layer, that is, an embedding matrix of dimensions (embedding dimension) × (number of embedded words). Tensorflow embedding lookups are performed to extract vector representations for words. The vocabulary distribution and the attention distribution only depend on attention, so we can easily work with altering only the embedding layer without affecting the rest of the architecture. The new model will learn new attention weights and encoder hidden states based on our new inputs. The code for our models is available online.\n          </p>\n          <h5>Notation</h5>\n          <p>\n            If the vector representation is dependent on context (like in ELMo), then <InlineMath>{\"v_{i,j}^{model}\"}</InlineMath> is the <InlineMath>{\"j^{th}\"}</InlineMath> word in the <InlineMath>{\"i^{th}\"}</InlineMath> sentence of some input sequence (the text to be summarized). <InlineMath>{\"model\"}</InlineMath> can take any of the following values <InlineMath>{\"ELMO_p, ELMO_t, GLOVE\"}</InlineMath> or <InlineMath>{\"W2V\"}</InlineMath>.\n          </p>\n          <p>\n            <InlineMath>{\"t\"}</InlineMath> indicates that there is some task-specific training that occurs when we change the embed- ding layer of the original text summarization model.\n          </p>\n          <h5>Word2Vec:Skipgram representations</h5>\n          <p>\n            At first we tried using pretrained embeddings by Google (Mikolov et. al, 2013) that were trained on Google articles but the problem we faced was that they only contained American English words and not British English (center not centre). This, along with other reasons, meant that more than 20 percent of our vocabulary was not in the embeddings. Therefore we decided to train our own skipgram model to get the embeddings. We trained skip gram embeddings with 40,000 articles (around 30 million words) for 300 dimension embeddings. We used a skip window of 1 (which could be varied in future testing). The loss function for this is noise contrastive estimation which aims to distinguish true context words from fake context words.\n          </p>\n          <h5>GloVe representations</h5>\n          <p>\n          We retrain See et al.’s (2017) network with the publicly available 300-dimensional pretrained vectors which have been trained on Gigaword 5 and Wikipedia. The intuition behind GloVe is that similar words should be close in vector space.\n          </p>\n          <h5>Task-Specific ELMo Representations</h5>\n          <p>\n            The GitHub repo for ELMo (allenlp library) states that \"Pre-trained contextual representations of words from large scale bidirectional language models provide large improvements over GloVe word2vec baselines.\" One main difference to note between GloVe and ELMo is that the former produces one unique representation for every word. For ELMo, we will input articles from the data into the pretrained model (as opposed to words) and receive a representation for each word in the article. In ELMo, context matters, and therefore the word \"dog\" in one context will not have the same representation as \"dog\" in another context. Peters et. al. (2018) explain that ELMo representations \"are computed on top of two-layer biLMs with character convolutions (Sec. 3.1), as a linear function of the internal network states.\" The following equation from Peters et al.’s (2018) paper describes the computation of a word representation:\n          </p>\n          <BlockMath>{\"v_{i,j}^{ELMO_t}=E(R_{i,j};\\\\Theta^t) = \\\\gamma^t \\\\sum_{j=0}^{L} s_l^t h_{l,i,j}^{LM} \"}</BlockMath>\n          <p>\n            <InlineMath>{\"\\\\Theta^t\"}</InlineMath> represents the parameters learned specifically for a specific task <InlineMath>{\"t\"}</InlineMath> (text summarization in our case). <InlineMath>{\"l\"}</InlineMath> denotes the layer in the ELMo architecture (starting at 0 for the input embedding).<InlineMath>{\"h_{l,i,j}^{LM}\"}</InlineMath> is a horizontal concatenation of the forward states and backward states (LSTM's) in ELMo for <InlineMath>{\"l \\> 0\"}</InlineMath>. These forward and backward states are language models. When <InlineMath>{\"l = 0\"}</InlineMath>, it is simply an embedded representation of a tokenized word. The final representation of the word is a weighted sum over all layers. These weights are learned in our task. Lastly, <InlineMath>{\"\\\\gamma\"}</InlineMath> is a scalar that multiplies our final vector, and is also task-specific learned parameter, as is <InlineMath>{\"s.\"}</InlineMath>\n          </p>\n          \n          <h5>Summary of Vectors Implemented</h5>\n          <div className={styles.tab} style={{width: \"80%\"}}>\n          <table>\n            <thead>\n              <tr>\n                <th>Model</th>\n                <th>Training Regimen</th>\n                <th>Pretrained Dataset</th>\n                <th>Embedding Dimension</th>\n              </tr>\n            </thead>\n            <tbody>\n              <tr>\n                <td>Embedding Matrix (Bengio et al., 2003)</td>\n                <td>Embedding matrix learned as part of the network</td>\n                <td>N/A</td>\n                <td>400</td>\n              </tr>\n              <tr>\n                <td>Skipgram (Mikolov et al., 2003)</td>\n                <td>Custom-trained on our training data with windowsize of 1</td>\n                <td>40,000 articles from our training data </td>\n                <td>300</td>\n              </tr>\n              <tr>\n                <td>GloVe (GloVe; Pennington et. al, 2014)</td>\n                <td>Publicly available pretrained vectors</td>\n                <td>Wikipedia 2014 + Gigaword 5</td>\n                <td>300</td>\n              </tr>\n              <tr>\n                <td>ELMo (Peters et al., 2018)</td>\n                <td>Learned task specific parameters for pretrained vectors</td>\n                <td>1 Billion Word Language Model Benchmark</td>\n                <td>1024</td>\n              </tr>\n            </tbody>\n          </table>\n          </div>\n        <h4>Experiments and Results</h4>\n          <h5>Settings</h5>\n            <p>\n              In order to take advantage of the most up-to-date CUDA libraries, we updated See’s original code to run on Tensorflow 1.13 and python 3.6. All models were built using See’s architecture beyond the input embedding portion. Due to the large cost in time and GPU memory, the training regimen and parameters were reduced. All models were run with a maximum encoding step size of 400, maximum decoding step size of 100, batch size of 8 and 128 hidden units for the LSTM layer. Based off See’s optimal vocabulary size finding, we set ours to 50,000 as well. All models were trained using the Adagrad optimizer with a learning rate of 0.15 and an initial accumulator value of 0.1.\n            </p>\n          <h5>Dataset</h5>\n            <p>\n              We trained all of our models using the CNN/Daily Mail dataset provided by See et al (2017). This corpus was originally produced by processing the DeepMind question and answer dataset presented by Hermann et al. (2015). The CNN/Daily Mail dataset contains news articles with their respective human made multi-sentence summaries separated into train, test and validation subsets. We only used 40% of the original training dataset, about 115,000 articles, in order to timely analyze all models. The test dataset was left untouched and it consisted of 11,490 articles.\n            </p>\n          <h5>Evaluation Metrics</h5>\n            <p>\n              Quantitatively, using the CNN/Daily Mail test dataset and the pyrougue library, we calculated ROUGE F1 scores for unigram, bigram and longest common subsequence. We then averaged the scores to give an overview of performance.\n            </p>\n            <p>\n              Qualitatively, we looked at different test samples and analyzed between different abstract outputs across different models for fluency, saliency, grammatical and information correctness, and abstractive strength.\n            </p>\n          <h5>Results</h5>\n            <p>\n              ROUGE F1 scores per different models trained under the same parameters. R-AVG is the average F1 score across all different ROUGE scores.\n            </p>\n            \n            <div className={styles.tab} style={{width: \"80%\"}}>\n              <table>\n                  <thead>\n                      <tr>\n                          <th>Model</th>\n                          <th>ROUGE-1</th>\n                          <th>ROUGE-2</th>\n                          <th>ROUGE-L</th>\n                          <th>R-AVG</th>\n                      </tr>\n                  </thead>\n                  <tbody>\n                      <tr>\n                        <td>\n                          PointerGenerator + Coverage - See et al (2017)\n                        </td>\n                        <td>\n                          39.53\n                        </td>\n                        <td>\n                          17.28\n                        </td>\n                        <td>\n                          36.38\n                        </td>\n                        <td>\n                          31.06\n                        </td>\n                      </tr>\n                      <tr>\n                        <td>\n                            PointerGenerator + Coverage - See et al (2017) - retrained\n                        </td>\n                        <td>\n                          35.37\n                        </td>\n                        <td>\n                          14.65\n                        </td>\n                        <td>\n                          31.80\n                        </td>\n                        <td>\n                          27.27\n                        </td>\n                      </tr>\n                      <tr>\n                        <td>\n                          Glove + PointerGenerator + Coverage\n                        </td>\n                        <td>\n                          36.00\n                        </td>\n                        <td>\n                          14.97\n                        </td>\n                        <td>\n                          33.04\n                        </td>\n                        <td>\n                          28.00\n                        </td>\n                      </tr>\n                      <tr>\n                        <td>\n                          SkipGram + PointerGenerator + Coverage\n                        </td>\n                        <td>\n                          37.00\n                        </td>\n                        <td>\n                          15.68\n                        </td>\n                        <td>\n                          33.58\n                        </td>\n                        <td>\n                          28.75\n                        </td>\n                      </tr>\n                      <tr>\n                        <td>\n                          ELMo + PointerGenerator + Coverage\n                        </td>\n                        <td>\n                          37.39\n                        </td>\n                        <td>\n                          15.81\n                        </td>\n                        <td>\n                          34.67\n                        </td>\n                        <td>\n                          29.29\n                        </td>\n                      </tr>\n                  </tbody>\n              </table>\n            </div>\n            <p>Randomly picked test articles: </p>\n            <div className={styles.tab} style={{width: \"90%\"}}>\n              <table>\n                  <thead>\n                    <tr>\n                      <th width=\"100%\">Article</th>\n                    </tr>\n                  </thead>\n                  <tbody>\n                    <tr>\n                      <td>\"this is the moment an unlucky man was gored between the legs by a rampaging bull after being knocked to the ground during a bullrunning celebration in spain . the unnamed man was sprinting ahead of the animal , trying to make it to the safety behind a set of iron bars when the angry animal sent him sprawling to the floor . and despite his desperate efforts to climb back up , the bull appeared to hit him right in the bottom with one of its horns as he tried to scramble away . scroll down for video . this unlucky man was gored during a bullrunning event in teulada , a small coastal town on spain ’s costa blanca , after falling over in front of the beast . the man , who has not been identified , was seen running away from the angry animal before it caught him with one horn , sending him sprawling to the floor . pictures show him crawling desperately towards the safety of the bull bars , but he was unable to make it before the horn hit him square between the legs . the unfortunate incident happened on the street in teulada , a small coastal town on spain ’s costa blanca , during the annual fiesta . the celebrations begin on april 8 and last until april 19 , and include floatillas and parades celebrating the town ’s patron saint , sant vicent ferrer . as part of the fiesta , large , aggressive bulls are let loose to run around the streets , with local and visitors invited to run alongside them . the challenge is to tease the animals before escaping behind one of the wooden pallets or iron bull bars to safety . witnesses said the man was seen limping towards medical crews after the attack , but was unsupported and appeared not to have been seriously hurt . the bullrunning takes place during the town ’s annual fiesta , which includes float parades in honour of sant vicent ferrer , the village ’s patron saint . websites advertising the bull running recommend leaving the sport to the experienced locals , noting that the animals are faster than people think , and that it is mostly foreigners that get injured . while it is not known where this man came from , he was seen limping away to be tended by medical crews after the attack . he was not though to have been seriously injured \"</td>\n                    </tr>\n                  </tbody>\n                  <thead>\n                    <tr>\n                      <th width=\"100%\">Human Summary</th>\n                    </tr>\n                  </thead>\n                  <tbody>\n                    <tr>\n                      <td>\"man was fleeing bull during fiesta celebrations in teulada , eastern spain . was knocked to the ground before crawling towards safety of bull bars . however he could n’t make it before horn caught him between the legs . seen limping towards medics , but was not thought to be seriously hurt .\"</td>\n                    </tr>\n                  </tbody>\n                  <thead>\n                    <tr>\n                      <th width=\"100%\">PointerGenerator + Coverage (baseline) Summary</th>\n                    </tr>\n                  </thead>\n                  <tbody>\n                    <tr>\n                      <td>\"the unnamed man was gored between the legs by a rampaging bull . the man was gored during a bullrunning event in teulada , a small coastal town on spain ’s costa blanca . the celebrations begin on april 8 and last until april 19 .\"'</td>\n                    </tr>\n                  </tbody>\n                  <thead>\n                    <tr>\n                      <th width=\"100%\">Glove + PointerGenerator + Coverage Summary</th>\n                    </tr>\n                  </thead>\n                  <tbody>\n                    <tr>\n                      <td>\"man was gored between the legs by a rampaging bull . the bull appeared to hit him right in the bottom with one of its horns as he tried to scramble away . despite his desperate efforts to climb back up , trying to make it to the safety behind a set of iron bars .\"</td>\n                    </tr>\n                  </tbody>\n                  <thead>\n                    <tr>\n                      <th width=\"100%\">SkipGram + PointerGenerator + Coverage Summary</th>\n                    </tr>\n                  </thead>\n                  <tbody>\n                    <tr>\n                      <td>\"unlucky man was gored during a bullrunning event in teulada , a small coastal town on spain ’s patron saint , sant vicent ferrer . witnesses said the man was seen limping towards medical crews after the town ’s patron saint . websites advertising the bull running recommend leaving the sport to run around the streets .\"</td>\n                   </tr>\n                  </tbody>\n                  <thead>\n                    <tr>\n                      <th width=\"100%\">ELMo + PointerGenerator + Coverage Summary</th>\n                    </tr>\n                  </thead>\n                  <tbody>\n                    <tr>\n                      <td>\"unlucky man was gored between the legs by a rampaging bull after being knocked to the ground during a bullrunning celebration in spain . the man was gored during a bullrunning event in teulada , a small coastal town on spain ’s costa blanca .\"</td>\n                    </tr>\n                  </tbody>\n              </table>\n              <br/>\n              <table>\n                  <thead>\n                    <tr>\n                      <th width=\"100%\">Article</th>\n                    </tr>\n                  </thead>\n                  <tbody>\n                    <tr>\n                      <td>\"jamie carragher believes chelsea captain john terry is the best defender to have ever graced the premier league . terry , 34 , has enjoyed another impressive season at stamford bridge and was the star performer as he led jose mourinho ’s side to a 0-0 draw against arsenal on sunday . the draw at the emirates means chelsea now need just six points from five games to claim their first premier league title since 2010 . john terry celebrates after the premier league match between arsenal and chelsea at the emirates stadium . jamie carragher believes the chelsea captain is the best defender to have played in the premier league . carragher insists terry deserves much of the credit , saying on sky sports : ‘ there ’s no doubt that , just behind -lrb- eden -rrb- hazard in this chelsea team , terry has been the most influential . ‘ we ’ve seen -lrb- cesc -rrb- fabregas , -lrb- nemanja -rrb- matic and diego costa feature in the early part of the season , but if you look at terry ’s display -lrb- against arsenal -rrb- he was outstanding . ‘ we talk about chelsea being a defensive team , but sometimes you ’ve got to say they defend brilliantly , and terry is the best . carragher -lrb- left -rrb- claimed terry has been almost as influential as chelsea ’s star man eden hazard . ‘ there have been a lot of great centre-backs in premier league history , but i think he has been the best that we ’ve seen . ’ blues boss mourinho was also quick to praise his captain after sunday ’s draw and even claimed it was terry ’s best performance in a chelsea shirt . mourinho said : ’ i told john terry in the dressing room that he made fantastic performances with me in six years but for me this was the best i have seen from him . ‘ it was the best jt has ever played . his defensive performance was absolutely amazing . ’ terry looks to get the better of arsenal striker olivier giroud during the 0-0 draw in north london . blues boss jose mourinho hailed terry ’s performance against arsenal as the‘ best ’ he has seen\"</td>\n                    </tr>\n                  </tbody>\n                  <thead>\n                    <tr>\n                      <th width=\"100%\">Human Summary</th>\n                    </tr>\n                  </thead>\n                  <tbody>\n                    <tr>\n                      <td>\"john terry helps chelsea earn 0-0 premier league draw against arsenal . chelsea captain has been in brilliant form for jose mourinho this season . jamie carragher believes terry is the premier league best centre back . mourinho : that was terry best performance for chelsea . if there was a pfa defender of the year category , terry would clean up !\"</td>\n                    </tr>\n                  </tbody>\n                  <thead>\n                    <tr>\n                      <th width=\"100%\">PointerGenerator + Coverage (baseline) Summary</th>\n                    </tr>\n                  </thead>\n                  <tbody>\n                    <tr>\n                      <td>\"chelsea captain john terry is the best defender to have ever graced the premier league . the draw at the emirates means chelsea now need just six points from five games to claim their first premier league title since 2010\"</td>     \n                    </tr>\n                  </tbody>\n                  <thead>\n                    <tr>\n                      <th width=\"100%\">Glove + PointerGenerator + Coverage Summary</th>\n                    </tr>\n                  </thead>\n                  <tbody>\n                    <tr>\n                      <td>\"jamie carragher believes chelsea captain john terry is the best defender . terry , 34 , has enjoyed another impressive season at stamford bridge . draw at the emirates means chelsea now need just six points from five games to claim their first premier league title since 2010 . blues boss jose mourinho hailed terry ’s performance against arsenal as ‘ best ’ he has seen .\"</td>\n                    </tr>\n                  </tbody>\n                  <thead>\n                    <tr>\n                      <th width=\"100%\">SkipGram + PointerGenerator + Coverage Summary</th>\n                    </tr>\n                  </thead>\n                  <tbody>\n                    <tr>\n                      <td>\"john terry celebrates after the premier league match between arsenal and chelsea at the emirates stadium . the draw at the emirates means chelsea now need just six points from five games to claim their first premier league title since 2010 .\"</td>\n                    </tr>\n                  </tbody>\n                  <thead>\n                    <tr>\n                      <th width=\"100%\">ELMo + PointerGenerator + Coverage Summary</th>\n                    </tr>\n                  </thead>\n                  <tbody>\n                    <tr>\n                      <td>\"jamie carragher believes chelsea captain john terry is the best defender to have ever graced the premier league . the draw at the emirates means chelsea now need just six points from five games to claim their first premier league title . the chelsea captain is the best defender to have played in the premier league .\"</td>\n                    </tr>\n                  </tbody>\n              </table>\n            </div>\n\n          <h5>Analysis</h5>\n            <p>\n              GloVe makes an improvement over the baseline for all ROUGE scores. When looking at the first example, we see that the baseline diverges from the main point in the last sentence. Clearly, the main point of the article is the man’s injury. However, the baseline says “the celebrations begin on april 8 and last until april 19 .” The GloVe summary focuses on the man’s injury all throughout, which is more relevant for the summarization task. The SkipGram summary attempts to connect two distant sentences from the article, but ultimately says something grammatical yet strange, stating \"websites advertising the bull running recommend leaving the sport to run around the streets.\" ELMo gives a a concise summary covering all important points, that is, the event, location, and some descriptive text. However, it makes no mention of the man \"scrambling away\" or seeking help.\n            </p>\n            <p>\n              In the second example, the GloVe model is able to quote Mourinho, which is also the case in the human summary. The baseline model, however, does not quote Mourinho, the manager for Chelsea. Mourinho’s comment on Terry is important. The GloVe model is more detailed than both the human summary and the baseline, stating both Terry’s age as well as the number of points Chelsea needs to win the Premiere League. SkipGram performs the worst. The difference between skipgram and the baseline is just two words; \"chelsea captain.\" Hence, the skipgram model is slightly (though negligibly) less detailed. Overall, GloVe gives us the best summary. However it is still less abstractive than the human summary. The ELMo summary, like the others, is grammatically correct and accurate, however, it repeats itself (first and last sentences).\n            </p>\n            <p>\n              At a glance, most other test abstracts results, across different models, had similar qualitative patterns as discussed above.\n            </p>\n            <p>\n            ROUGE scores across different implementations underwent similar consistent improvements at the word level, bigram level and longest sequence level. The ELMo based model, implementing a context based approach, enjoyed the largest rise in ROUGE score. While these improvements seem encouraging, ROUGE scores as a metric have been shown to be less than ideal due to the subjective manner in which they favor the reference —human made abstract in our case (Shi et al 2018). Hence while our scores showed improvement, they don’t capture the full picture as well as our qualitative analysis.\n            </p>\n        <h4>Conclusion and Future Work</h4>\n        <p>\n          In this work, we presented different pointer-generator + coverage models implementing novel embeddings developed in the past two decade (Almeida and Xexéo, 2019). These models managed to outperform the baseline — using an embedding matrix — in both ROUGE scores and qualitative results. In the same vein as ELMo, other transfer learning models based off sentence context (such as BERT) could also improve the performance on these metrics. Nonetheless, these models seem to hit a wall as to the level of salient information, grammatical correctness and human readability produced. We believe that delving beyond character, word and sentence level embeddings is needed to achieve this. Developing paragraph and article embeddings to complement the embeddings used in this paper could be a path forword to further improve text summarization.\n        </p>\n        <h4>References</h4>\n        <p>See, A., Liu, P. J., & Manning, C. D. (2017). Get to the point: Summarization with pointer-generator networks. arXiv preprint arXiv:1704.04368. Retrieved from https:arxiv.org/abs/1704.04368</p>\n        <p>Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L. (2018). Deep contextualized word representations. arXiv preprint arXiv:1802.05365.</p>\n        <p>Pennington, J., Socher, R., Manning, C. (2014). Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) (pp. 1532-1543).</p>\n        <p>Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.</p>\n        <p>Hermann, K. M., Kocisky, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M., & Blunsom, P. (2015). Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems (pp. 1693-1701).</p>\n        <p>Mikolov, T., Chen, K., Corrado, G., Dean, J. (2013). \"Efficient estimation of word representations in vector space.\" arXiv preprint arXiv:1301.3781.</p>\n        <p>Bengio, Y., Ducharme, R., Vincent, P., Jauvin, C. (2003). A neural probabilistic language model. Journal of machine learning research, 3(Feb), 1137-1155.</p>\n        <p>Almeida, F., Xexéo, G. (2019). Word Embeddings: A Survey. arXiv preprint arXiv:1901.09069.</p>\n        <p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Polosukhin, I. (2017). Attention is all you need. CoRR abs/1706.03762.</p>\n        <p>Shi, Tian, et al. \"Neural Abstractive Text Summarization with Sequence-to-Sequence Models.\" arXiv preprint arXiv:1812.02303 (2018).</p>\n        <hr></hr>\n        <hr></hr>\n      </Route>\n      \n      <div className={styles.listEnt}>\n        <Link to='/research/research-2'>\n          <h3>Deep Learning Resource Analysis Using Memory-Efficient Adaptive Optimization</h3>\n          <p>with Rahul Das and Zhixiang Hu</p>\n          <hr/>\n        </Link>\n      </div>\n      <Route path='/research/research-2'> \n        <div className={styles.but}>\n          <a href=\"https://github.com/rahuldas1/SM3/tree/pers\">code &raquo;</a>\n          <a href=\"https://drive.google.com/file/d/1qIKxtGb02S-qkhoqftMCb0Ezpjo3X3Fl/view?usp=sharing\">paper &raquo;</a>\n        </div>\n        <hr/>\n        <h4>Abstract</h4>\n        <p>\n          The use of adaptive gradient-based optimizers is widespread in machine learning, the most popular of which are Adam, Adagrad, and Adafactor. These optimizers utilize cumulative second-order statistics to tune the learning rate of each parameter during the optimization process, presenting numerous advantages, most importantly, constraining the time and space requirements of the methods to be linear in the number of parameters. However, the existing optimizers still present significant memory overhead when training models with billions of parameters. This places a limitation on the size of the model and on the batch size during training, which can severely effect the accuracy of the model. By improving on the memory overhead of existing optimizers, we can train ever more complex and accurate models, which is especially pertinent to the field of natural language processing. Anil et al. [2] present a novel adaptive optimization method which retains the benefits of conventional per-parameter adaptivity while significantly reducing memory requirements.\n        </p>\n        <h4>Introduction</h4>\n        <p>\n          The use of adaptive gradient-based optimizers is widespread in machine learning, the most popular of which are Adam, Adagrad, and Adafactor. These optimizers utilize cumulative second-order statistics to tune the learning rate of each parameter during the optimization process, presenting numerous advantages, most importantly, constraining the time and space requirements of the methods to be linear in the number of parameters.\n        </p>\n        <p>\n          However, the existing optimizers still present significant memory overhead when training models with billions of parameters. This places a limitation on the size of the model and on the batch size during training, which can severely effect the accuracy of the model. By improving on the memory overhead of existing optimizers, we can train ever more complex and accurate models, which is especially pertinent to the field of natural language processing. Anil et al. [2] present a novel adaptive optimization method which retains the benefits of conventional per-parameter adaptivity while significantly reducing memory requirements.\n        </p>\n        <p>\n          The optimization algorithm, presented below in pseudocode (SM3-I), utilizes covers over the parameters, i.e. a collection of k nonempty set <InlineMath>{\"\\\\{S_r\\\\}_{r=1}^k\"}</InlineMath> such that <InlineMath>{\"S_r \\\\subseteq [d]\"}</InlineMath> and <InlineMath>{\"\\\\cup_rS_r = [d].\"}</InlineMath> For each set <InlineMath>{\"S_r\"}</InlineMath> in the cover, the algorithm maintains a cumulative sum <InlineMath>{\"\\\\mu_t(r)\"}</InlineMath> of the maximal variance over all gradient entries <InlineMath>{\"j\\\\in S_r.\"}</InlineMath> Then, for each parameter <InlineMath>{\"i,\"}</InlineMath> the minimum is taken over all variables <InlineMath>{\"\\\\mu_t(r)\"}</InlineMath> associated with sets which cover <InlineMath>{\"i(S_r \\\\ni i).\"}</InlineMath> The square root of this minimum, <InlineMath>{\"\\\\sqrt{v_t(i)}\"}</InlineMath>, is used to determine the learning rate corresponding to the <InlineMath>{\"i\"}</InlineMath>'th gradient entry. As such, the algorithm is named Square-root of Minima of Sums of Maxima of Squared-gradients Method, or simply, SM3.\n        </p>\n        <div className={styles.algo}>\n          <table>\n            <thead>\n              <tr style={{textAlign: \"center\"}}>\n                SM3-I <InlineMath>{\"(\\\\eta)\"}</InlineMath> Algorithm\n              </tr>\n            </thead>\n            <tbody>\n              <tr>\n                <td style={{paddingLeft: \"40px\"}}>\n                <b>Input:</b> learning rate <InlineMath>{\"\\\\eta\"}</InlineMath>\n                </td>\n              </tr>\n              <tr>\n                <td style={{paddingLeft: \"40px\"}}>Initialize <InlineMath>{\"w_1 = 0; \\\\forall r \\\\in [k] : \\\\mu_0(r) = 0;\"}</InlineMath></td>\n              </tr>\n              <tr>\n                <td style={{paddingLeft: \"40px\"}}><b>for </b> <InlineMath>{\"t = 1,\\\\cdots, T \"}</InlineMath><b> do:</b></td> \n              </tr>\n              <tr>\n                <td style={{paddingLeft: \"80px\"}}>receive gradient <InlineMath>{\"g_t = \\\\nabla \\\\ell_t(w_t)\"}</InlineMath></td>\n              </tr>\n              <tr>\n                <td style={{paddingLeft: \"80px\"}}><b>for </b> <InlineMath>{\"r = 1,\\\\cdots, k \"}</InlineMath><b> do:</b></td>\n              </tr>\n              <tr>\n                <td style={{paddingLeft: \"120px\"}}>set <InlineMath>{\"\\\\mu_t(r) \\\\leftarrow \\\\mu_{t-1}(r) +\\\\max_{j\\\\in S_r}g_t^2(j)\"}</InlineMath></td>\n              </tr>\n              <tr>\n                <td style={{paddingLeft: \"80px\"}}><b>end for</b></td>\n              </tr>\n              <tr>\n                <td style={{paddingLeft: \"80px\"}}><b>for </b> <InlineMath>{\"i = 1,\\\\cdots, d \"}</InlineMath><b> do:</b></td>\n              </tr>\n              <tr>\n                <td style={{paddingLeft: \"120px\"}}>set <InlineMath>{\"\\\\nu_t(r) \\\\leftarrow \\\\min_{r:S_r\\\\ni i}\\\\mu_t(r)\"}</InlineMath></td>\n              </tr>\n              <tr>\n                <td style={{paddingLeft: \"80px\"}}><b>end for</b></td>\n              </tr>\n              <tr>\n                <td style={{paddingLeft: \"80px\"}}>update <InlineMath>{\"w_{t+1}(i) \\\\leftarrow w_t(i) - \\\\eta g_t(i)/\\\\sqrt{\\\\nu_t(i)}\"}</InlineMath></td>\n              </tr>\n              <tr>\n                <td style={{paddingLeft: \"40px\"}}><b>end for</b></td>\n              </tr>\n            </tbody>\n          </table>\n        </div>\n        <p>\n          They also propose a variant of, SM3-II, which they claim provides a tighter upper bound on the cumulative gradient squares, as compared to SM3-I.\n        </p>\n        <div className={styles.algo}>\n          <table>\n            <thead>\n              <tr style={{textAlign: \"center\"}}>\n                SM3-II <InlineMath>{\"(\\\\eta)\"}</InlineMath> Algorithm\n              </tr>\n            </thead>\n            <tbody>\n              <tr>\n                <td style={{paddingLeft: \"40px\"}}>\n                <b>Input:</b> learning rate <InlineMath>{\"\\\\eta\"}</InlineMath>\n                </td>\n              </tr>\n              <tr>\n                <td style={{paddingLeft: \"40px\"}}>Initialize <InlineMath>{\"w_1 = 0; \\\\forall r \\\\in [k] : \\\\mu'_0(r) = 0;\"}</InlineMath></td>\n              </tr>\n              <tr>\n                <td style={{paddingLeft: \"40px\"}}><b>for </b> <InlineMath>{\"t = 1,\\\\cdots, T \"}</InlineMath><b> do:</b></td> \n              </tr>\n              <tr>\n                <td style={{paddingLeft: \"80px\"}}>receive gradient <InlineMath>{\"g_t = \\\\nabla \\\\ell_t(w_t)\"}</InlineMath></td>\n              </tr>\n              <tr>\n                <td style={{paddingLeft: \"80px\"}}>Initialize <InlineMath>{\"\\\\forall r \\\\in [k] : \\\\mu'_t(r) = 0\"}</InlineMath></td>\n              </tr>\n              <tr>\n                <td style={{paddingLeft: \"80px\"}}><b>for </b> <InlineMath>{\"i = 1,\\\\cdots, d \"}</InlineMath><b> do:</b></td>\n              </tr>\n              <tr>\n                <td style={{paddingLeft: \"120px\"}}>set <InlineMath>{\"\\\\nu'_t(i) \\\\leftarrow \\\\min_{r:S_r \\\\ni i}\\\\mu'_{t-1}(r) + g_t^2(j)\"}</InlineMath></td>\n              </tr>\n              <tr>\n                <td style={{paddingLeft: \"120px\"}}>set <InlineMath>{\"w'_{t+1}(i) \\\\leftarrow w_{t}(i) - \\\\eta g_t(i)/\\\\sqrt{\\\\nu'_t(i)}\"}</InlineMath>*</td>\n              </tr>\n              <tr>\n                <td style={{paddingLeft: \"120px\"}}>*with the convention that <InlineMath>{\"0/0 =0\"}</InlineMath></td>\n              </tr>\n              <tr>\n                <td style={{paddingLeft: \"120px\"}}><b>for </b> all <InlineMath>{\"r:S_r \\\\ni i\"}</InlineMath><b> do:</b></td>\n              </tr>\n              <tr>\n                <td style={{paddingLeft: \"160px\"}}>set <InlineMath>{\"\\\\mu'_t(r) \\\\leftarrow \\\\max\\\\{\\\\mu'_t(r), \\\\nu'_t(i)\\\\}\"}</InlineMath></td>\n              </tr>\n              <tr>\n                <td style={{paddingLeft: \"120px\"}}><b>end for</b></td>\n              </tr>\n              <tr>\n                <td style={{paddingLeft: \"80px\"}}><b>end for</b></td>\n              </tr>\n              <tr>\n                <td style={{paddingLeft: \"40px\"}}><b>end for</b></td>\n              </tr>\n            </tbody>\n          </table>\n        </div>\n        <h4>Experimental Setup</h4>\n        <p>\n          In the following sections, we will compare the performance of SM3 with other adaptive (Adam, Adagrad) and non- adaptive (SGD) optimizers.\n        </p>\n        <p>\n          To test the optimizers, we consider a language modelling task, specifically dependency parsing. For this task, the model must parse the sentence into a tree structure by correctly labelling each word with the correct part-of-speech and predicting the relationship between words in the sentence. The model used is a feed-forward neural network with an embedding layer and two hidden layers, which produces a categorical output.\n        </p>\n        <p>\n          The dataset used is the Penn Treebank WSJ, which is split into training (∼40,000 sentences) and test (∼2,500 sentences) sets. Each optimizer considered is initialized with a learning rate = 0.01. For evaluation, we consider the training loss, training categorical accuracy, CPU usage, and runtime. To evaluate the model’s accuracy in the task of dependency parsing, we use macro (sentence-based) Labelled Attachment Score (LAS) and Unlabelled Attachment Score (UAS), which provide useful information about how frequently the model correctly labels words and their relations to other words in the sentence.\n        </p>\n        <div className={styles.fig}>\n          <img src={rTwoFigOne}/>\n          <p>Fig, 1.: <i>Comparison of SM3, Adagrad, and Adam across 10 training epochs. Upper figure shows training with batch size=64, and lower figure shows batch size=512</i></p>\n         \n        </div>\n         \n        <h4>Experimental Results</h4>\n          <p>\n            We first consider the training loss over the course of 10 training steps (epochs), as shown in Fig. 1. It can be seen that the SM3-optimized models have significantly higher training loss at the end of the first few epochs, after which the results are comparable with Adam and Adagrad. In the case of small batch size (64), training loss decreases rapidly, eventually reaching the level of Adagrad and outperforming Adam. However, when training with large batch size (512), convergence is slower and significantly worse than both Adam and Adagrad.\n          </p>\n          <div className={styles.tab} style={{width: \"80%\"}}>\n          <table style={{width:\"30%\", marginLeft: \"auto\", marginRight:\"auto\" }}>\n            <thead>\n              <tr>\n                <td>Optimizer</td>\n                <td>Batch Size</td>\n                <td>CPU%</td>\n                <td>Runtime (mins)</td>\n              </tr>\n            </thead>\n            <tbody>\n              <tr>\n                <td>SM3</td>\n                <td>64</td>\n                <td>38.3</td>\n                <td>8.5</td>\n              </tr>\n              <tr>\n                <td>SGD</td>\n                <td>64</td>\n                <td>40.6</td>\n                <td>9.1</td>\n              </tr>\n              <tr>\n                <td>Adagrad</td>\n                <td>64</td>\n                <td>42.8</td>\n                <td>10.3</td>\n              </tr>\n              <tr>\n                <td>Adam</td>\n                <td>64</td>\n                <td>53.7</td>\n                <td>13.1</td>\n              </tr>\n              <tr>\n                <td>SM3</td>\n                <td>512</td>\n                <td>46.6</td>\n                <td>2.8</td>\n              </tr>\n              <tr>\n                <td>SGD</td>\n                <td>512</td>\n                <td>54.1</td>\n                <td>3.9</td>\n              </tr>\n              <tr>\n                <td>Adagrad</td>\n                <td>512</td>\n                <td>54.1</td>\n                <td>3.9</td>\n              </tr>\n              <tr>\n                <td>Adam</td>\n                <td>512</td>\n                <td>58.1</td>\n                <td>4.0</td>\n              </tr>\n            </tbody>\n          </table>\n          <p style={{fontSize: \"14px\"}}>TABLE I: <i>Comparison of SM3, SGD, AdaGrad, and Adam based on CPU usage and runtime (training).</i></p>\n          \n          </div>\n          <p>\n            We now consider the time requirements of the various optimizers. Table I shows the CPU% and runtime when training the model for 10 epochs. SM3 presents significant time savings over other optimizers, up to 25% in some cases. From the results, it is clear that SM3 is less computationally expensive than the adaptive optimizers Adam and Adagrad, and is comparable to standard SGD.\n          </p>\n          <div className={styles.tab} style={{width: \"80%\"}}>\n          <table style={{width:\"30%\", marginLeft: \"auto\", marginRight:\"auto\" }}>\n            <thead>\n              <tr>\n                <td>Optimizer</td>\n                <td>LAS</td>\n                <td>UAS</td>\n              </tr>\n            </thead>\n            <tbody>\n              <tr>\n                <td>SM3</td>\n                <td>0.732</td>\n                <td>0.783</td>\n              </tr>\n              <tr>\n                <td>SGD</td>\n                <td>0.727</td>\n                <td>0.778</td>\n              </tr>\n              <tr>\n                <td>Adagrad</td>\n                <td>0.713</td>\n                <td>0.769</td>\n              </tr>\n              <tr>\n                <td>Adam</td>\n                <td>0.745</td>\n                <td>0.794</td>\n              </tr>\n            </tbody>\n          </table>\n\n          <p style={{fontSize: \"14px\"}}>TABLE II: <i>Performance of SM3, SGD, AdaGrad, and Adam on test set. Metrics used are macro labelled attachment score (LAS) and macro unlabelled attachment score (UAS).</i></p>\n          </div>\n          <p>\n            Finally, we evaluate the actual performance of the models trained with different optimizers. The metrics used measure the frequency with which the model accurately labels words and identifies the parent word (’head’) in the dependency tree. Specifically, LAS measures the percentage of the time that words are assigned the correct head AND label, and UAS measures the percentage of the time that words are assigned the correct head. The dependency parser used for this task performs adequately as compared to state-of-the- art dependency parsers, which often achieve LAS and UAS values exceeding 0.900. From Table II, it can be seen that SM3 performs as well or better than other optimizers, with the exception of Adam.\n          </p>\n        <h4>Evaluation</h4>\n        <p>\n          The experiments results show that SM3 presents significant savings over the usage of CPU. For example, there’s a 28.68% reduction when compared to Adam in the case of batch size 64. This verifies the authors’ claim that SM3 has memory efficiency over other adaptive algorithms. The better utilization of memory allowed the CPU to be used more efficiently.\n        </p>\n        <p>  \n          When we compare the losses of different algorithms, we also found that as the number of epoch increases, SM3 tends to converge to the performance of Adagrad. This is also consistent with the result shown in Proposition 1 from the paper (or Proposition 3 in section V I below) that SM3 has a convergence bound that is of the same order of magnitude as Adagrad’s.\n        </p>\n        <h4>Conclusion</h4>\n          <p>Based on experiments with dataset, SM3 exhibits superior efficiency and performance compared to existing adaptive optimizers and SGD.</p>\n        <h4>Anaylysis of SM3-I and SM3-II</h4>\n        <p>Here we give a summary of the convergence properties of the algorithms and show that SM3 − II has a tighter regret bound than SM3 − I. We start with some preliminary results proved in [1, Proposition 3] and [2, Claim 2, Proposition 3].</p>\n        <p><b>Proposition 1.</b><i> Assume that the loss functions</i> <InlineMath>{\"\\\\ell_1, \\\\ell_2, \\\\cdots\"}</InlineMath><i> are convex, and let </i><InlineMath>{\"w_1, w_2, \\\\cdots\"}</InlineMath><i>be the iterates generated by SM3-I or SM3-II. Let </i><InlineMath>{\"H_t=diag(\\\\nu_t^{1/2})\"}</InlineMath><i>Assume</i> <InlineMath>{\"\\\\max_t\\\\Vert w_t - w^*\\\\Vert_\\\\infty^2 \\\\leq D.\"}</InlineMath><i> Then for any </i><InlineMath>{\"w^* \\\\in \\\\mathbb{R}^d\"}</InlineMath><i>, the regret of SM3-I or SM3-II is bounded by:</i></p> \n        <BlockMath>{\"\\\\frac{1}{2\\\\eta} \\\\sum_{t=1}^T (\\\\Vert w_t-w^* \\\\Vert_{H_t}^2 - \\\\Vert w_{t+1} - w^* \\\\Vert_{H_t}^2) + \\\\frac{\\\\eta}{2} \\\\sum_{t=1}^T (\\\\Vert g_t\\\\Vert_{H_t}^*)^2\"}</BlockMath>\n        <p><i>where </i><InlineMath>{\"\\\\Vert x\\\\Vert_H^2\"}</InlineMath> <i>is defined as </i><InlineMath>{\"\\\\sqrt{x^THx}\"}</InlineMath><i> and </i><InlineMath>{\"\\\\Vert x\\\\Vert_H^*\"}</InlineMath><i> as </i><InlineMath>{\"\\\\sqrt{x^Th^{-1}x}\"}</InlineMath>.</p>\n        <p><i>Proof.</i> See Duchi e., 2011: Propositon 3</p>\n        <p><b>Proposition 2.</b> <i> For any </i><InlineMath>{\"i \\\\in [d],\"}</InlineMath><i> the sequences </i><InlineMath>{\"\\\\nu'_1(i), \\\\nu'_2(i), \\\\cdots,\"}</InlineMath><i> from SM3-II are monotonically increasing. Fix a sequence of gradients </i><InlineMath>{\"g_1, g_2,\\\\cdots\"}</InlineMath><i> we have that for all </i><InlineMath>t, i</InlineMath><i> that </i><InlineMath>{\"\\\\sum_{s=1}^t g_s^2(i) \\\\leq \\\\nu'_t(i) \\\\leq \\\\nu_t(i).\"}</InlineMath></p>\n        <p><i>Proof. </i>See Anil et., 2019: Claim 2 and Proposition 3.</p>\n        <p><b>Proposition 3.</b><i>Given the setup in Proposition 1, let </i><InlineMath>{\"w_1, w_2,\\\\cdots\"}</InlineMath><i> be the iterates generated by SM3-I and </i><InlineMath>{\"w'_1,w'_2,\\\\cdots\"}</InlineMath><i> be the iterates from SM3-II. Let also </i><InlineMath>{\"H'_t\"}</InlineMath><i> be the diagonal matrix contains the sequence values </i><InlineMath>{\"\\\\nu'_1(i), \\\\nu'_2(i),\\\\cdots,\"}</InlineMath><i> and set the learning rate </i><InlineMath>{\"\\\\eta = D.\"}</InlineMath><i> Then we have the following convergence bounds for the two algorithms: </i></p>\n        <BlockMath>{\"\\\\sum_{t=1}^T (\\\\ell_t w'_t - \\\\ell_t w^*) \\\\leq \"}</BlockMath>\n        <BlockMath>{\"\\\\frac{3D}{2} \\\\sum_{t=1}^d \\\\sqrt{\\\\sum_{t=1}^T \\\\big[ g_t^2(i) + \\\\min_{r:S_r \\\\ni i} \\\\max\\\\{\\\\mu'_{t-1}(r),\\\\nu'_{t-1}(i) \\\\}\\\\big]} \"}</BlockMath>\n        <BlockMath>{\"\\\\leq \\\\sum_{t=1}^T (\\\\ell_t w_t - \\\\ell_t w^*) \\\\leq\"}</BlockMath>\n        <BlockMath>{\"\\\\frac{3D}{2} \\\\sum_{t=1}^T \\\\sqrt{ \\\\min_{r:S_r \\\\ni i} \\\\sum_{t=1}^T \\\\max_{j\\\\in S_r} g_t^2(j)}\"}</BlockMath>\n        <p><i>with </i><InlineMath>{\"\\\\mu_0(r) = \\\\nu_0(r) = \\\\mu'_0(r) = \\\\nu'_0(r) = 0\"}</InlineMath></p>\n        <p><i>Proof. </i> From Proposition 1, we have:</p>\n        <BlockMath>{\"\\\\sum_{t=1}^T (\\\\ell_t w_t - \\\\ell_t w^*) \\\\leq\"}</BlockMath>\n        <BlockMath>{\"\\\\frac{1}{2\\\\eta}\\\\sum_{t=1}^T [\\\\Vert w_t - w^*\\\\Vert_{H_t}^2 - \\\\Vert w_{t+1} - w^*\\\\Vert_{H_t}^2] + \\\\frac{\\\\eta}{2} \\\\sum_{t=1}^T [\\\\Vert g_t\\\\Vert_{H_t}^*]^2\"}</BlockMath>\n        <p>The first term on the RHS can be bounded as follows:</p>\n        <BlockMath>{\"2\\\\eta (\\\\mathrm{I}) \\\\leq \\\\sum_{t=1}^T (\\\\nu_t^{1/2} - \\\\nu_{t-1}^{1/2})(w_t-w^*)^2\"}</BlockMath>\n        <BlockMath>{\"\\\\leq \\\\sum_{t=1}^T (\\\\nu_t^{1/2} - \\\\nu_{t-1}^{1/2})(\\\\Vert w_t-w^*\\\\Vert_\\\\infty^2 \\\\cdot \\\\mathbb{1}_d)\"}</BlockMath>\n        <BlockMath>{\"\\\\leq  D^2(\\\\nu_T^{1/2}\\\\cdot \\\\mathbb{1}_d = D^2 \\\\ \\\\mathrm{Tr}(H_T)\"}</BlockMath>\n        <p>For the second term, let <InlineMath>{\"\\\\gamma_t(i) = \\\\sum_{s=1}^t g_s^2(i)\"}</InlineMath> and consider the positive definite diagonal matrix <InlineMath>{\"G_t = \\\\mathrm{diag}(\\\\gamma_t^{1/2})\"}</InlineMath>. Use the results from [3, Lemma 2] with <InlineMath>{\"\\\\Phi(G) = \\\\mathrm{Tr}(G)\"}</InlineMath>, we have:</p>\n        <BlockMath>{\"\\\\sum_{t=1}^T (\\\\Vert g_t\\\\Vert_{G_t}^*)^2 \\\\leq \\\\sum_{t=1}^T (\\\\Vert g_t\\\\Vert_{G_T}^*)^2 + \\\\mathrm{Tr}(G_T)\"}</BlockMath>\n        <BlockMath>{\"= \\\\gamma_T^{-1/2} \\\\cdot \\\\gamma_T = 2 \\\\ \\\\mathrm{Tr}(G_T)\"}</BlockMath>\n        <p>Now from Proposition 2, we know for all <InlineMath>{\"t , H_t \\\\succeq H'_t \\\\succeq G_t.\"}</InlineMath> Then we have: </p>\n        <BlockMath>{\"\\\\frac{2}{\\\\eta}(\\\\mathrm{II})\\\\leq \\\\sum_{t=1}^T (\\\\Vert g_t\\\\Vert_{G_t}^*)^2 \\\\leq 2\\\\mathrm{Tr}(G_T) \\\\leq 2\\\\mathrm{Tr}(H'_T) \\\\leq 2\\\\mathrm{Tr}(H_T)\"}</BlockMath>\n        <p>Then we have:</p>\n        <BlockMath>{\"\\\\sum_{t=1}^T (\\\\ell_t w_t - \\\\ell_t w^*) \\\\leq \\\\Bigg( \\\\frac{D^2}{2\\\\eta} + \\\\eta \\\\Bigg) \\\\mathrm{Tr}(H_T) = \\\\frac{3}{2}D \\\\ \\\\mathrm{Tr}(H_T)\"}</BlockMath>\n        <BlockMath>{\"\\\\sum_{t=1}^T (\\\\ell_t w'_t - \\\\ell_t w^*) \\\\leq \\\\frac{3}{2}D \\\\ \\\\mathrm{Tr}(H'_T) \\\\leq \\\\frac{3}{2}D \\\\ \\\\mathrm{Tr}(H_T)\"}</BlockMath>\n        <p>Plug in the definitions for <InlineMath>{\"\\\\nu_T\"}</InlineMath> and <InlineMath>{\"\\\\nu'_T\"}</InlineMath> we then recover then results stated in the Proposition</p>\n        <h4>Bibliography</h4>\n        <p>[1] J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121–2159, 2011.</p>\n        <p>[2] R. Anil, V. Gupta, T. Koren, Y. Singer. Memory-Efficient Adaptive Optimization. https://arxiv.org/abs/1901.11150.</p>\n        <p>[3] V. Gupta, T. Koren, and Y. Singer. Shampoo: Precon- ditioned stochastic tensor optimization. In Proceedings of the 35th International Conference on Machine Learning, volume 80, pages 1842–1850, 2018.</p>\n        <hr></hr>\n        <hr></hr>\n      </Route>\n      </div>\n    </div>\n  );\n};\nexport default App;","// This optional code is used to register a service worker.\n// register() is not called by default.\n\n// This lets the app load faster on subsequent visits in production, and gives\n// it offline capabilities. However, it also means that developers (and users)\n// will only see deployed updates on subsequent visits to a page, after all the\n// existing tabs open on the page have been closed, since previously cached\n// resources are updated in the background.\n\n// To learn more about the benefits of this model and instructions on how to\n// opt-in, read https://bit.ly/CRA-PWA\n\nconst isLocalhost = Boolean(\n  window.location.hostname === 'localhost' ||\n    // [::1] is the IPv6 localhost address.\n    window.location.hostname === '[::1]' ||\n    // 127.0.0.0/8 are considered localhost for IPv4.\n    window.location.hostname.match(\n      /^127(?:\\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}$/\n    )\n);\n\nexport function register(config) {\n  if (process.env.NODE_ENV === 'production' && 'serviceWorker' in navigator) {\n    // The URL constructor is available in all browsers that support SW.\n    const publicUrl = new URL(process.env.PUBLIC_URL, window.location.href);\n    if (publicUrl.origin !== window.location.origin) {\n      // Our service worker won't work if PUBLIC_URL is on a different origin\n      // from what our page is served on. This might happen if a CDN is used to\n      // serve assets; see https://github.com/facebook/create-react-app/issues/2374\n      return;\n    }\n\n    window.addEventListener('load', () => {\n      const swUrl = `${process.env.PUBLIC_URL}/service-worker.js`;\n\n      if (isLocalhost) {\n        // This is running on localhost. Let's check if a service worker still exists or not.\n        checkValidServiceWorker(swUrl, config);\n\n        // Add some additional logging to localhost, pointing developers to the\n        // service worker/PWA documentation.\n        navigator.serviceWorker.ready.then(() => {\n          console.log(\n            'This web app is being served cache-first by a service ' +\n              'worker. To learn more, visit https://bit.ly/CRA-PWA'\n          );\n        });\n      } else {\n        // Is not localhost. Just register service worker\n        registerValidSW(swUrl, config);\n      }\n    });\n  }\n}\n\nfunction registerValidSW(swUrl, config) {\n  navigator.serviceWorker\n    .register(swUrl)\n    .then(registration => {\n      registration.onupdatefound = () => {\n        const installingWorker = registration.installing;\n        if (installingWorker == null) {\n          return;\n        }\n        installingWorker.onstatechange = () => {\n          if (installingWorker.state === 'installed') {\n            if (navigator.serviceWorker.controller) {\n              // At this point, the updated precached content has been fetched,\n              // but the previous service worker will still serve the older\n              // content until all client tabs are closed.\n              console.log(\n                'New content is available and will be used when all ' +\n                  'tabs for this page are closed. See https://bit.ly/CRA-PWA.'\n              );\n\n              // Execute callback\n              if (config && config.onUpdate) {\n                config.onUpdate(registration);\n              }\n            } else {\n              // At this point, everything has been precached.\n              // It's the perfect time to display a\n              // \"Content is cached for offline use.\" message.\n              console.log('Content is cached for offline use.');\n\n              // Execute callback\n              if (config && config.onSuccess) {\n                config.onSuccess(registration);\n              }\n            }\n          }\n        };\n      };\n    })\n    .catch(error => {\n      console.error('Error during service worker registration:', error);\n    });\n}\n\nfunction checkValidServiceWorker(swUrl, config) {\n  // Check if the service worker can be found. If it can't reload the page.\n  fetch(swUrl, {\n    headers: { 'Service-Worker': 'script' },\n  })\n    .then(response => {\n      // Ensure service worker exists, and that we really are getting a JS file.\n      const contentType = response.headers.get('content-type');\n      if (\n        response.status === 404 ||\n        (contentType != null && contentType.indexOf('javascript') === -1)\n      ) {\n        // No service worker found. Probably a different app. Reload the page.\n        navigator.serviceWorker.ready.then(registration => {\n          registration.unregister().then(() => {\n            window.location.reload();\n          });\n        });\n      } else {\n        // Service worker found. Proceed as normal.\n        registerValidSW(swUrl, config);\n      }\n    })\n    .catch(() => {\n      console.log(\n        'No internet connection found. App is running in offline mode.'\n      );\n    });\n}\n\nexport function unregister() {\n  if ('serviceWorker' in navigator) {\n    navigator.serviceWorker.ready\n      .then(registration => {\n        registration.unregister();\n      })\n      .catch(error => {\n        console.error(error.message);\n      });\n  }\n}\n","import React from 'react';\nimport ReactDOM from 'react-dom';\nimport './index.css';\nimport App from './App';\nimport * as serviceWorker from './serviceWorker';\n\nReactDOM.render(\n  <React.StrictMode>\n    <App />\n  </React.StrictMode>,\n  document.getElementById('root')\n);\n\n// If you want your app to work offline and load faster, you can change\n// unregister() to register() below. Note this comes with some pitfalls.\n// Learn more about service workers: https://bit.ly/CRA-PWA\nserviceWorker.unregister();\n"],"sourceRoot":""}